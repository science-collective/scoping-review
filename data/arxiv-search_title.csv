"","id","title","abstract","authors","link_pdf","link_doi"
"1","0904.3950v1","New Science on the Open Science Grid","  The Open Science Grid (OSG) includes work to enable new science, new
scientists, and new modalities in support of computationally based research.
There are frequently significant sociological and organizational changes
required in transformation from the existing to the new. OSG leverages its
deliverables to the large scale physics experiment member communities to
benefit new communities at all scales through activities in education,
engagement and the distributed facility. As a partner to the poster and
tutorial at SciDAC 2008, this paper gives both a brief general description and
some specific examples of new science enabled on the OSG. More information is
available at the OSG web site: (http://www.opensciencegrid.org).
","The Open Science Grid Executive Board| :|Ruth Pordes|Mine Altunay|Paul Avery|Alina Bejan|Kent Blackburn|Alan Blatecky|Rob Gardner|Bill Kramer|Miron Livny|John McGee|Maxim Potekhin|Rob Quick|Doug Olson|Alain Roy|Chander Sehgal|Torre Wenaus|Mike Wilde|Frank Wuerthwein","http://arxiv.org/pdf/0904.3950v1","http://dx.doi.org/10.1088/1742-6596/125/1/012070"
"2","1210.0713v1","Open Science Project in White Dwarf Research","  I will propose a new way of advancing white dwarf research. Open science is a
method of doing research that lets everyone who has something to say about the
subject take part in the problem solving process.
  Already now, the amount of information we gather from observations, theory
and modelling is too vast for any one individual to comprehend and turn into
knowledge. And the amount of information just keeps growing in the future. A
platform that promotes sharing of thoughts and ideas allows us to pool our
collective knowledge of white dwarfs and get a clear picture of our research
field. It will also make it possible for researchers in fields closely related
to ours (AGB stars, planetary nebulae etc.) to join the scientific discourse.
  In the first stage this project would allow us to summarize what we know and
what we don't, and what we should search for next. Later, it could grow into a
large collaboration that would have the impact to, for example, suggest
instrument requirements for future telescopes to satisfy the needs of the white
dwarf community, or propose large surveys.
  A simple implementation would be a wiki page for collecting knowledge
combined with a forum for more extensive discussions. These would be simple and
cheap to maintain. A large community effort on the whole would be needed for
the project to succeed, but individual workload should stay at a low level.
","Tommi Vornanen","http://arxiv.org/pdf/1210.0713v1",""
"3","1308.0843v2","Snowmass Energy Frontier Simulations using the Open Science Grid (A
  Snowmass 2013 whitepaper)","  Snowmass is a US long-term planning study for the high-energy community by
the American Physical Society's Division of Particles and Fields. For its
simulation studies, opportunistic resources are harnessed using the Open
Science Grid infrastructure. Late binding grid technology, GlideinWMS, was used
for distributed scheduling of the simulation jobs across many sites mainly in
the US. The pilot infrastructure also uses the Parrot mechanism to dynamically
access CvmFS in order to ascertain a homogeneous environment across the nodes.
This report presents the resource usage and the storage model used for
simulating large statistics Standard Model backgrounds needed for Snowmass
Energy Frontier studies.
","A. Avetisyan|S. Bhattacharya|M. Narain|S. Padhi|J. Hirschauer|T. Levshina|P. McBride|C. Sehgal|M. Slyz|M. Rynge|S. Malik|J. Stupak III","http://arxiv.org/pdf/1308.0843v2",""
"4","1308.5533v1","Climate Change and Open Science","  Obtaining reliable answers to the major scientific questions raised by
climate change in time to take appropriate action gives added urgency to the
open access program.
","Ian Percival","http://arxiv.org/pdf/1308.5533v1",""
"5","1309.2966v1","Sustainable Software Ecosystems for Open Science","  Sustainable software ecosystems are difficult to build, and require concerted
effort, community norms and collaborations. In science it is especially
important to establish communities in which faculty, staff, students and
open-source professionals work together and treat software as a first-class
product of scientific investigation-just as mathematics is treated in the
physical sciences. Kitware has a rich history of establishing collaborative
projects in the science, engineering and medical research fields, and continues
to work on improving that model as new technologies and approaches become
available. This approach closely follows and is enhanced by the movement
towards practicing open, reproducible research in the sciences where data,
source code, methodology and approach are all available so that complex
experiments can be independently reproduced and verified.
","Marcus D. Hanwell|Amitha Perera|Wes Turner|Patrick O'Leary|Katie Osterdahl|Bill Hoffman|Will Schroeder","http://arxiv.org/pdf/1309.2966v1","http://dx.doi.org/10.6084/m9.figshare.790756"
"6","1311.4762v2","Software Uncertainty in Integrated Environmental Modelling: the role of
  Semantics and Open Science","  Computational aspects increasingly shape environmental sciences. Actually,
transdisciplinary modelling of complex and uncertain environmental systems is
challenging computational science (CS) and also the science-policy interface.
Large spatial-scale problems falling within this category - i.e. wide-scale
transdisciplinary modelling for environment (WSTMe) - often deal with factors
(a) for which deep-uncertainty may prevent usual statistical analysis of
modelled quantities and need different ways for providing policy-making with
science-based support. Here, practical recommendations are proposed for
tempering a peculiar - not infrequently underestimated - source of uncertainty.
Software errors in complex WSTMe may subtly affect the outcomes with possible
consequences even on collective environmental decision-making. Semantic
transparency in CS and free software are discussed as possible mitigations.
","Daniele de Rigo","http://arxiv.org/pdf/1311.4762v2","http://dx.doi.org/10.6084/m9.figshare.155701"
"7","1402.6013v1","Open science in machine learning","  We present OpenML and mldata, open science platforms that provides easy
access to machine learning data, software and results to encourage further
study and application. They go beyond the more traditional repositories for
data sets and software packages in that they allow researchers to also easily
share the results they obtained in experiments and to compare their solutions
with those of others.
","Joaquin Vanschoren|Mikio L. Braun|Cheng Soon Ong","http://arxiv.org/pdf/1402.6013v1",""
"8","1404.0191v2","A quantitative perspective on ethics in large team science","  The gradual crowding out of singleton and small team science by large team
endeavors is challenging key features of research culture. It is therefore
important for the future of scientific practice to reflect upon the individual
scientist's ethical responsibilities within teams. To facilitate this
reflection we show labor force trends in the US revealing a skewed growth in
academic ranks and increased levels of competition for promotion within the
system; we analyze teaming trends across disciplines and national borders
demonstrating why it is becoming difficult to distribute credit and to avoid
conflicts of interest; and we use more than a century of Nobel prize data to
show how science is outgrowing its old institutions of singleton awards. Of
particular concern within the large team environment is the weakening of the
mentor-mentee relation, which undermines the cultivation of virtue ethics
across scientific generations. These trends and emerging organizational
complexities call for a universal set of behavioral norms that transcend team
heterogeneity and hierarchy. To this end, our expository analysis provides a
survey of ethical issues in team settings to inform science ethics education
and science policy.
","Alexander M. Petersen|Ioannis Pavlidis|Ioanna Semendeferi","http://arxiv.org/pdf/1404.0191v2","http://dx.doi.org/10.1007/s11948-014-9562-8"
"9","1406.7541v1","Open Collaboration for Innovation: Principles and Performance","  The principles of open collaboration for innovation (and production), once
distinctive to open source software, are now found in many other ventures. Some
of these ventures are internet-based: Wikipedia, online forums and communities.
Others are off-line: in medicine, science, and everyday life. Such ventures
have been affecting traditional firms, and may represent a new organizational
form. Despite the impact of such ventures, questions remain about their
operating principles and performance. Here we define open collaboration (OC),
the underlying set of principles, and propose that it is a robust engine for
innovation and production. First, we review multiple OC ventures and identify
four defining principles. In all instances, participants create goods and
services of economic value, they exchange and reuse each other's work, they
labor purposefully with just loose coordination, and they permit anyone to
contribute and consume. These principles distinguish OC from other
organizational forms, such as firms or cooperatives. Next, we turn to
performance. To understand the performance of OC, we develop a computational
model, combining innovation theory with recent evidence on human cooperation.
We identify and investigate three elements that affect performance: the
cooperativeness of participants, the diversity of their needs, and the degree
to which the goods are rival (subtractable). Through computational experiments,
we find that OC performs well even in seemingly harsh environments: when
cooperators are a minority, free riders are present, diversity is lacking, or
goods are rival. We conclude that OC is viable and likely to expand into new
domains. The findings also inform the discussion on new organizational forms,
collaborative and communal.
","Sheen Levine|Michael Prietula","http://arxiv.org/pdf/1406.7541v1",""
"10","1407.3682v2","When data sharing gets close to 100%: what ancient human DNA studies can
  teach the Open Science movement","  This study analyzes rates and ways of data sharing regarding mitochondrial, Y
chromosomal and autosomal polymorphisms in a total of 162 papers on human
ancient DNA published between 1988 and 2013. For the most part, data are
available in such a way as to make their scrutiny and reuse possible. The
estimated sharing rate is not far from totality (97.6% +/- 2.1%) and
substantially higher than observed in other fields of genetic research
(Evolutionary, Medical and Forensic Genetics). A questionnaire-based survey
suggests that the authors awareness of the importance of openness and
transparency for scientific progress is a fundamental factor for the
achievement of such a high sharing rate. Most data were made available through
body text, but the use of primary databases increased with the application of
complete mitochondrial and next generation sequencing methods. Our study
highlights three important aspects. First, we provide evidence that researchers
motivations are as necessary as stakeholders policies and norms to achieve very
high sharing rates. Second, careful analyses of the ways in which data are made
available are an important first step to maximize data findability,
accessibility, useability and preservation. Third and finally, the case of
human ancient DNA studies demonstrates how Open Science can foster scientific
advancements, showing that openness and transparency can help build rigorous
and reliable scientific practices even in the presence of complex experimental
challenges.
","Paolo Anagnostou|Marco Capocasa|Nicola Milia|Emanuele Sanna|Daniela Luzi|Giovanni Destro Bisol","http://arxiv.org/pdf/1407.3682v2","http://dx.doi.org/10.1371/journal.pone.0121409"
"11","1410.4839v2","The LIGO Open Science Center","  The LIGO Open Science Center (LOSC) fulfills LIGO's commitment to release,
archive, and serve LIGO data in a broadly accessible way to the scientific
community and to the public, and to provide the information and tools necessary
to understand and use the data. In August 2014, the LOSC published the full
dataset from Initial LIGO's ""S5"" run at design sensitivity, the first such
large-scale release and a valuable testbed to explore the use of LIGO data by
non-LIGO researchers and by the public, and to help teach gravitational-wave
data analysis to students across the world. In addition to serving the S5 data,
the LOSC web portal (losc.ligo.org) now offers documentation, data-location and
data-quality queries, tutorials and example code, and more. We review the
mission and plans of the LOSC, focusing on the S5 data release.
","Michele Vallisneri|Jonah Kanner|Roy Williams|Alan Weinstein|Branson Stephens","http://arxiv.org/pdf/1410.4839v2","http://dx.doi.org/10.1088/1742-6596/610/1/012021"
"12","1506.05632v1","An Open Science Platform for the Next Generation of Data","  Imagine an online work environment where researchers have direct and
immediate access to myriad data sources and tools and data management
resources, useful throughout the research lifecycle. This is our vision for the
next generation of the Dataverse Network: an Open Science Platform (OSP). For
the first time, researchers would be able to seamlessly access and create
primary and derived data from a variety of sources: prior research results,
public data sets, harvested online data, physical instruments, private data
collections, and even data from other standalone repositories. Researchers
could recruit research participants and conduct research directly on the OSP,
if desired, using readily available tools. Researchers could create private or
shared workspaces to house data, access tools, and computation and could
publish data directly on the platform or publish elsewhere with persistent,
data citations on the OSP. This manuscript describes the details of an Open
Science Platform and its construction. Having an Open Science Platform will
especially impact the rate of new scientific discoveries and make scientific
findings more credible and accountable.
","Latanya Sweeney|Merce Crosas","http://arxiv.org/pdf/1506.05632v1",""
"13","1601.00323v1","The Design of a Community Science Cloud: The Open Science Data Cloud
  Perspective","  In this paper we describe the design, and implementation of the Open Science
Data Cloud, or OSDC. The goal of the OSDC is to provide petabyte-scale data
cloud infrastructure and related services for scientists working with large
quantities of data. Currently, the OSDC consists of more than 2000 cores and 2
PB of storage distributed across four data centers connected by 10G networks.
We discuss some of the lessons learned during the past three years of operation
and describe the software stacks used in the OSDC. We also describe some of the
research projects in biology, the earth sciences, and social sciences enabled
by the OSDC.
","Robert L. Grossman|Matthew Greenway|Allison P. Heath|Ray Powell|Rafael D. Suarez|Walt Wells|Kevin White|Malcolm Atkinson|Iraklis Klampanos|Heidi L. Alvarez|Christine Harvey|Joe J. Mambretti","http://arxiv.org/pdf/1601.00323v1",""
"14","1610.00652v1","Open research areas in distance geometry","  Distance Geometry is based on the inverse problem that asks to find the
positions of points, in a Euclidean space of given dimension, that are
compatible with a given set of distances. We briefly introduce the field, and
discuss some open and promising research areas.
","Leo Liberti|Carlile Lavor","http://arxiv.org/pdf/1610.00652v1",""
"15","1701.03507v1","Beyond NGS data sharing and towards open science","  Biosciences have been revolutionized by next generation sequencing (NGS)
technologies in last years, leading to new perspectives in medical, industrial
and environmental applications. And although our motivation comes from
biosciences, the following is true for many areas of science: published results
are usually hard to reproduce either because data is not available or tools are
not readily available, which delays the adoption of new methodologies and
hinders innovation. Our focus is on tool readiness and pipelines availability.
Even though most tools are freely available, pipelines for data analysis are in
general barely described and their configuration is far from trivial, with many
parameters to be tuned.
  In this paper we discuss how to effectively build and use pipelines, relying
on state of the art computing technologies to execute them without users need
to configure, install and manage tools, servers and complex workflow management
systems. We perform an in depth comparative analysis of state of the art
frameworks and systems. The NGSPipes framework is proposed showing that we can
have public pipelines ready to process and analyse experimental data, produced
for instance by high-throughput technologies, but without relying on
centralized servers or Web services.
  The NGSPipes framework and underlying architecture provides a major step
towards open science and true collaboration in what concerns tools and
pipelines among computational biology researchers and practitioners. We show
that it is possible to execute data analysis pipelines in a decentralized and
platform independent way. Approaches like the one proposed are crucial for
archiving and reusing data analysis pipelines at medium/long-term. NGSPipes
framework is freely available at http://ngspipes.github.io/.
","Bruno Dantas|Calmenelias Fleitas|Alexandre P. Francisco|José Simão|Cátia Vaz","http://arxiv.org/pdf/1701.03507v1",""
"16","1702.01251v1","Cyber-Physical Attacks on UAS Networks- Challenges and Open Research
  Problems","  Assignment of critical missions to unmanned aerial vehicles (UAV) is bound to
widen the grounds for adversarial intentions in the cyber domain, potentially
ranging from disruption of command and control links to capture and use of
airborne nodes for kinetic attacks. Ensuring the security of electronic and
communications in multi-UAV systems is of paramount importance for their safe
and reliable integration with military and civilian airspaces. Over the past
decade, this active field of research has produced many notable studies and
novel proposals for attacks and mitigation techniques in UAV networks. Yet, the
generic modeling of such networks as typical MANETs and isolated systems has
left various vulnerabilities out of the investigative focus of the research
community. This paper aims to emphasize on some of the critical challenges in
securing UAV networks against attacks targeting vulnerabilities specific to
such systems and their cyber-physical aspects.
","Vahid Behzadan","http://arxiv.org/pdf/1702.01251v1",""
"17","1702.04855v1","Open Science, Public Engagement and the University","  Contemporary debates on ""open science"" mostly focus on the pub- lic
accessibility of the products of scientific and academic work. In contrast,
this paper presents arguments for ""opening"" the ongoing work of science. That
is, this paper is an invitation to rethink the university with an eye toward
engaging the public in the dynamic, conceptual and representational work
involved in creating scientific knowledge. To this end, we posit that public
computing spaces, a genre of open- ended, public learning environment where
visitors interact with open source computing platforms to directly access,
modify and create complex and authentic scientific work, can serve as a
possible model of ""open science"" in the university.
","Pratim Sengupta|Marie-Claire Shanahan","http://arxiv.org/pdf/1702.04855v1",""
"18","1704.02319v2","BEAT: An Open-Source Web-Based Open-Science Platform","  With the increased interest in computational sciences, machine learning (ML),
pattern recognition (PR) and big data, governmental agencies, academia and
manufacturers are overwhelmed by the constant influx of new algorithms and
techniques promising improved performance, generalization and robustness.
Sadly, result reproducibility is often an overlooked feature accompanying
original research publications, competitions and benchmark evaluations. The
main reasons behind such a gap arise from natural complications in research and
development in this area: the distribution of data may be a sensitive issue;
software frameworks are difficult to install and maintain; Test protocols may
involve a potentially large set of intricate steps which are difficult to
handle. Given the raising complexity of research challenges and the constant
increase in data volume, the conditions for achieving reproducible research in
the domain are also increasingly difficult to meet.
  To bridge this gap, we built an open platform for research in computational
sciences related to pattern recognition and machine learning, to help on the
development, reproducibility and certification of results obtained in the
field. By making use of such a system, academic, governmental or industrial
organizations enable users to easily and socially develop processing
toolchains, re-use data, algorithms, workflows and compare results from
distinct algorithms and/or parameterizations with minimal effort. This article
presents such a platform and discusses some of its key features, uses and
limitations. We overview a currently operational prototype and provide design
insights.
","André Anjos|Laurent El-Shafey|Sébastien Marcel","http://arxiv.org/pdf/1704.02319v2",""
"19","1709.08767v1","BOSS-LDG: A Novel Computational Framework that Brings Together Blue
  Waters, Open Science Grid, Shifter and the LIGO Data Grid to Accelerate
  Gravitational Wave Discovery","  We present a novel computational framework that connects Blue Waters, the
NSF-supported, leadership-class supercomputer operated by NCSA, to the Laser
Interferometer Gravitational-Wave Observatory (LIGO) Data Grid via Open Science
Grid technology. To enable this computational infrastructure, we configured,
for the first time, a LIGO Data Grid Tier-1 Center that can submit
heterogeneous LIGO workflows using Open Science Grid facilities. In order to
enable a seamless connection between the LIGO Data Grid and Blue Waters via
Open Science Grid, we utilize Shifter to containerize LIGO's workflow software.
This work represents the first time Open Science Grid, Shifter, and Blue Waters
are unified to tackle a scientific problem and, in particular, it is the first
time a framework of this nature is used in the context of large scale
gravitational wave data analysis. This new framework has been used in the last
several weeks of LIGO's second discovery campaign to run the most
computationally demanding gravitational wave search workflows on Blue Waters,
and accelerate discovery in the emergent field of gravitational wave
astrophysics. We discuss the implications of this novel framework for a wider
ecosystem of Higher Performance Computing users.
","E. A. Huerta|Roland Haas|Edgar Fajardo|Daniel S. Katz|Stuart Anderson|Peter Couvares|Josh Willis|Timothy Bouvet|Jeremy Enos|William T. C. Kramer|Hon Wai Leong|David Wheeler","http://arxiv.org/pdf/1709.08767v1","http://dx.doi.org/10.1109/eScience.2017.47"
"20","1710.05246v3","Shared High Value Research Resources: The CamCAN Human Lifespan
  Neuroimaging Dataset Processed on the Open Science Grid","  The CamCAN Lifespan Neuroimaging Dataset, Cambridge (UK) Centre for Ageing
and Neuroscience, was acquired and processed beginning in December, 2016. The
referee consensus solver deployed to the Open Science Grid was used for this
task. The dataset includes demographic and screening measures, a
high-resolution MRI scan of the brain, and whole-head magnetoencephalographic
(MEG) recordings during eyes closed rest (560 sec), a simple task (540 sec),
and passive listening/viewing (140 sec). The data were collected from 619
neurologically normal individuals, ages 18-87. The processed results from the
resting recordings are completed and available online. These constitute 1.7
TBytes of data including the location within the brain (1 mm resolution), time
stamp (1 msec resolution), and 80 msec time course for each of 3.7 billion
validated neuroelectric events, i.e. mean 6.1 million events for each of the
619 participants.
  The referee consensus solver provides high yield (mean 11,000 neuroelectric
currents/sec; standard deviation (sd): 3500/sec) high confidence (p < 10-12 for
each identified current) measures of the neuroelectric currents whose magnetic
fields are detected in the MEG recordings. We describe the solver, the
implementation of the solver deployed on the Open Science Grid, the workflow
management system, the opportunistic use of high performance computing (HPC)
resources to add computing capacity to the Open Science Grid reserved for this
project, and our initial findings from the recently completed processing of the
resting recordings. This required 14 million core hours, i.e. 40 core hours per
second of data.
","Don Krieger|Paul Shepard|Ben Zusman|Anirban Jana|David O. Okonkwo","http://arxiv.org/pdf/1710.05246v3",""
"21","1801.02634v2","The Astropy Project: Building an inclusive, open-science project and
  status of the v2.0 core package","  The Astropy project supports and fosters the development of open-source and
openly-developed Python packages that provide commonly-needed functionality to
the astronomical community. A key element of the Astropy project is the core
package Astropy, which serves as the foundation for more specialized projects
and packages. In this article, we provide an overview of the organization of
the Astropy project and summarize key features in the core package as of the
recent major release, version 2.0. We then describe the project infrastructure
designed to facilitate and support development for a broader ecosystem of
inter-operable packages. We conclude with a future outlook of planned new
features and directions for the broader Astropy project.
"," The Astropy Collaboration|A. M. Price-Whelan|B. M. Sipőcz|H. M. Günther|P. L. Lim|S. M. Crawford|S. Conseil|D. L. Shupe|M. W. Craig|N. Dencheva|A. Ginsburg|J. T. VanderPlas|L. D. Bradley|D. Pérez-Suárez|M. de Val-Borro|T. L. Aldcroft|K. L. Cruz|T. P. Robitaille|E. J. Tollerud|C. Ardelean|T. Babej|M. Bachetti|A. V. Bakanov|S. P. Bamford|G. Barentsen|P. Barmby|A. Baumbach|K. L. Berry|F. Biscani|M. Boquien|K. A. Bostroem|L. G. Bouma|G. B. Brammer|E. M. Bray|H. Breytenbach|H. Buddelmeijer|D. J. Burke|G. Calderone|J. L. Cano Rodríguez|M. Cara|J. V. M. Cardoso|S. Cheedella|Y. Copin|D. Crichton|D. DÁvella|C. Deil|É. Depagne|J. P. Dietrich|A. Donath|M. Droettboom|N. Earl|T. Erben|S. Fabbro|L. A. Ferreira|T. Finethy|R. T. Fox|L. H. Garrison|S. L. J. Gibbons|D. A. Goldstein|R. Gommers|J. P. Greco|P. Greenfield|A. M. Groener|F. Grollier|A. Hagen|P. Hirst|D. Homeier|A. J. Horton|G. Hosseinzadeh|L. Hu|J. S. Hunkeler|Ž. Ivezić|A. Jain|T. Jenness|G. Kanarek|S. Kendrew|N. S. Kern|W. E. Kerzendorf|A. Khvalko|J. King|D. Kirkby|A. M. Kulkarni|A. Kumar|A. Lee|D. Lenz|S. P. Littlefair|Z. Ma|D. M. Macleod|M. Mastropietro|C. McCully|S. Montagnac|B. M. Morris|M. Mueller|S. J. Mumford|D. Muna|N. A. Murphy|S. Nelson|G. H. Nguyen|J. P. Ninan|M. Nöthe|S. Ogaz|S. Oh|J. K. Parejko|N. Parley|S. Pascual|R. Patil|A. A. Patil|A. L. Plunkett|J. X. Prochaska|T. Rastogi|V. Reddy Janga|J. Sabater|P. Sakurikar|M. Seifert|L. E. Sherbert|H. Sherwood-Taylor|A. Y. Shih|J. Sick|M. T. Silbiger|S. Singanamalla|L. P. Singer|P. H. Sladen|K. A. Sooley|S. Sornarajah|O. Streicher|P. Teuben|S. W. Thomas|G. R. Tremblay|J. E. H. Turner|V. Terrón|M. H. van Kerkwijk|A. de la Vega|L. L. Watkins|B. A. Weaver|J. B. Whitmore|J. Woillez|V. Zabalza","http://arxiv.org/pdf/1801.02634v2","http://dx.doi.org/10.3847/1538-3881/aabc4f"
"22","1802.04068v1","Towards an Open Science Platform for the Evaluation of Data Fusion","  Combining the results of different search engines in order to improve upon
their performance has been the subject of many research papers. This has become
known as the ""Data Fusion"" task, and has great promise in dealing with the vast
quantity of unstructured textual data that is a feature of many Big Data
scenarios. However, no universally-accepted evaluation methodology has emerged
in the community. This makes it difficult to make meaningful comparisons
between the various proposed techniques from reading the literature alone.
Variations in the datasets, metrics, and baseline results have all contributed
to this difficulty.
  This paper argues that a more unified approach is required, and that a
centralised software platform should be developed to aid researchers in making
comparisons between their algorithms and others. The desirable qualities of
such a system have been identified and proposed, and an early prototype has
been developed. Re-implementing algorithms published by other researchers is a
great burden on those proposing new techniques. The prototype system has the
potential to greatly reduce this burden and thus encourage more comparable
results being generated and published more easily.
","Weinan Huang|Junyi Chen|Lei Meng|David Lillis","http://arxiv.org/pdf/1802.04068v1",""
"23","1803.10281v2","Authentication schemes for Smart Mobile Devices: Threat Models,
  Countermeasures, and Open Research Issues","  This paper presents a comprehensive investigation of authentication schemes
for smart mobile devices. We start by providing an overview of existing survey
articles published in the recent years that deal with security for mobile
devices. Then, we describe and give a classification of threat models in smart
mobile devices in five categories, including, identity-based attacks,
eavesdropping-based attacks, combined eavesdropping and identity-based attacks,
manipulation-based attacks, and service-based attacks. We also provide a
classification of countermeasures into four types of categories, including,
cryptographic functions, personal identification, classification algorithms,
and channel characteristics. According to these, we categorize authentication
schemes for smart mobile devices in four categories, namely, 1) biometric-based
authentication schemes, 2) channel-based authentication schemes, 3)
factor-based authentication schemes, and 4) ID-based authentication schemes. In
addition, we provide a taxonomy and comparison of authentication schemes for
smart mobile devices in the form of tables. Finally, we identify open
challenges and future research directions.
","Mohamed Amine Ferrag|Leandros Maglaras|Abdelouahid Derhab|Helge Janicke","http://arxiv.org/pdf/1803.10281v2",""
"24","1804.05492v1","Using the Jupyter Notebook as a Tool for Open Science: An Empirical
  Study","  As scientific work becomes more computational and data intensive, research
processes and results become more difficult to interpret and reproduce. In this
poster, we show how the Jupyter notebook, a tool originally designed as a free
version of Mathematica notebooks, has evolved to become a robust tool for
scientists to share code, associated computation, and documentation.
","Bernadette M. Boscoe|Irene V. Pasquetto|Milena S. Golshan|Christine L. Borgman","http://arxiv.org/pdf/1804.05492v1",""
"25","1807.06639v1","Discovering Job Preemptions in the Open Science Grid","  The Open Science Grid(OSG) is a world-wide computing system which facilitates
distributed computing for scientific research. It can distribute a
computationally intensive job to geo-distributed clusters and process job's
tasks in parallel. For compute clusters on the OSG, physical resources may be
shared between OSG and cluster's local user-submitted jobs, with local jobs
preempting OSG-based ones. As a result, job preemptions occur frequently in
OSG, sometimes significantly delaying job completion time.
  We have collected job data from OSG over a period of more than 80 days. We
present an analysis of the data, characterizing the preemption patterns and
different types of jobs. Based on observations, we have grouped OSG jobs into 5
categories and analyze the runtime statistics for each category. we further
choose different statistical distributions to estimate probability density
function of job runtime for different classes.
","Zhe Zhang|Brian Bockelman|Derek Weitzel|David Swanson","http://arxiv.org/pdf/1807.06639v1","http://dx.doi.org/10.1145/3219104.3229282"
"26","1808.07282v2","Empowering open science with reflexive and spatialised indicators","  Bibliometrics have become commonplace and widely used by authors and journals
to monitor, to evaluate and to identify their readership in an
ever-increasingly publishing scientific world. With this contribution, we aim
to investigate the semantic proximities and evolution of the papers published
in the online journal Cybergeo since its creation in 1996. We propose a
dedicated interactive application that compares three strategies for building
semantic networks, using keywords (self-declared themes), citations (areas of
research using the papers published in Cybergeo) and full-texts (themes derived
from the words used in writing). We interpret these networks and semantic
proximities with respect to their temporal evolution as well as to their
spatial expressions, by considering the countries studied in the papers under
inquiry (Cybergeo being a journal of geography, most articles refer to a
well-defined spatial envelope). Finally, we compare the three methods and
conclude that their complementarity can help go beyond simple statistics to
better understand the epistemological evolution of a scientific community and
the readership target of the journal.
","Juste Raimbault|Pierre-Olivier Chasset|Clémentine Cottineau|Hadrien Commenges|Denise Pumain|Christine Kosmopoulos|Arnaud Banos","http://arxiv.org/pdf/1808.07282v2",""
"27","1808.08070v1","The Open Energy Modelling Framework (oemof) - A new approach to
  facilitate open science in energy system modelling","  Energy system models have become indispensable tools for planning future
energy systems by providing insights into different development trajectories.
However, sustainable systems with high shares of renewable energy are
characterized by growing cross-sectoral interdependencies and decentralized
structures. To capture important properties of increasingly complex energy
systems, sophisticated and flexible modelling tools are needed. At the same
time, open science is becoming increasingly important in energy system
modelling. This paper presents the Open Energy Modelling Framework (oemof) as a
novel approach to energy system modelling, representation and analysis. The
framework provides a toolbox to construct comprehensive energy system models
and has been published open source under a free licence. Through collaborative
development based on open processes, the framework supports a maximum level of
participation, transparency and open science principles in energy system
modelling. Based on a generic graph-based description of energy systems, it is
well-suited to flexibly model complex cross-sectoral systems and incorporate
various modelling approaches. This makes the framework a multi-purpose
modelling environment for modelling and analyzing different systems at scales
ranging from urban to transnational.
","Simon Hilpert|Cord Kaldemeyer|Uwe Krien|Stefan Günther|Clemens Wingenbach|Guido Plessmann","http://arxiv.org/pdf/1808.08070v1","http://dx.doi.org/10.1016/j.esr.2018.07.001"
"28","1812.04298v1","Text data mining and data quality management for research information
  systems in the context of open data and open science","  In the implementation and use of research information systems (RIS) in
scientific institutions, text data mining and semantic technologies are a key
technology for the meaningful use of large amounts of data. It is not the
collection of data that is difficult, but the further processing and
integration of the data in RIS. Data is usually not uniformly formatted and
structured, such as texts and tables that cannot be linked. These include
various source systems with their different data formats such as project and
publication databases, CERIF and RCD data model, etc. Internal and external
data sources continue to develop. On the one hand, they must be constantly
synchronized and the results of the data links checked. On the other hand, the
texts must be processed in natural language and certain information extracted.
Using text data mining, the quality of the metadata is analyzed and this
identifies the entities and general keywords. So that the user is supported in
the search for interesting research information. The information age makes it
easier to store huge amounts of data and increase the number of documents on
the internet, in institutions' intranets, in newswires and blogs is
overwhelming. Search engines should help to specifically open up these sources
of information and make them usable for administrative and research purposes.
Against this backdrop, the aim of this paper is to provide an overview of text
data mining techniques and the management of successful data quality for RIS in
the context of open data and open science in scientific institutions and
libraries, as well as to provide ideas for their application. In particular,
solutions for the RIS will be presented.
","Otmane Azeroual|Gunter Saake|Mohammad Abuosba|Joachim Schöpfel","http://arxiv.org/pdf/1812.04298v1",""
"29","1812.10175v1","The iEnvironment Platform: Developing an Open Science Software Platform
  for Integrated Environmental Monitoring and Modeling of Surface Water","  This paper describes the development of iEnvironment, an open science
software platform that supports monitoring and modeling of aspects of surface
water. The platform supports science and engineering research, especially in
the context of the creation, sharing, analysis and maintenance of big and open
data. In this era of big data, iEnvironment facilitates access to open data
resources and research collaboration among science and research disciplines
supported by computer scientists and software developers.
","Paulo Alencar|Donald Cowan|Doug Mulholland","http://arxiv.org/pdf/1812.10175v1",""
"30","1901.10816v3","Open Research Knowledge Graph: Next Generation Infrastructure for
  Semantic Scholarly Knowledge","  Despite improved digital access to scholarly knowledge in recent decades,
scholarly communication remains exclusively document-based. In this form,
scholarly knowledge is hard to process automatically. In this paper, we present
the first steps towards a knowledge graph based infrastructure that acquires
scholarly knowledge in machine actionable form thus enabling new possibilities
for scholarly knowledge curation, publication and processing. The primary
contribution is to present, evaluate and discuss multi-modal scholarly
knowledge acquisition, combining crowdsourced and automated techniques. We
present the results of the first user evaluation of the infrastructure with the
participants of a recent international conference. Results suggest that users
were intrigued by the novelty of the proposed infrastructure and by the
possibilities for innovative scholarly knowledge processing it could enable.
","Mohamad Yaser Jaradeh|Allard Oelen|Kheir Eddine Farfar|Manuel Prinz|Jennifer D'Souza|Gábor Kismihók|Markus Stocker|Sören Auer","http://arxiv.org/pdf/1901.10816v3",""
"31","1903.01403v1","Towards A Methodology and Framework for Workflow-Driven Team Science","  Scientific workflows are powerful tools for management of scalable
experiments, often composed of complex tasks running on distributed resources.
Existing cyberinfrastructure provides components that can be utilized within
repeatable workflows. However, data and computing advances continuously change
the way scientific workflows get developed and executed, pushing the scientific
activity to be more data-driven, heterogeneous and collaborative. Workflow
development today depends on the effective collaboration and communication of a
cross-disciplinary team, not only with humans but also with analytical systems
and infrastructure. This paper presents a collaboration-centered reference
architecture to extend workflow systems with dynamic, predictable and
programmable interfaces to systems and infrastructure while bridging the
exploratory and scalable activities in the scientific process. We also present
a conceptual design towards the development of methodologies and tools for
effective workflow-driven collaborations, namely the PPoDS methodology and the
SmartFlows Toolkit for smart utilization of workflows in a rapidly evolving
cyberinfrastructure ecosystem.
","Ilkay Altintas|Shweta Purawat|Daniel Crawl|Alok Singh|Kyle Marcus","http://arxiv.org/pdf/1903.01403v1",""
"32","1902.10265v2","A Vision of 6G Wireless Systems: Applications, Trends, Technologies, and
  Open Research Problems","  The ongoing deployment of 5G cellular systems is continuously exposing the
inherent limitations of this system, compared to its original premise as an
enabler for Internet of Everything applications. These 5G drawbacks are
currently spurring worldwide activities focused on defining the next-generation
6G wireless system that can truly integrate far-reaching applications ranging
from autonomous systems to extended reality and haptics. Despite recent 6G
initiatives1, the fundamental architectural and performance components of the
system remain largely undefined. In this paper, we present a holistic,
forward-looking vision that defines the tenets of a 6G system. We opine that 6G
will not be a mere exploration of more spectrum at high-frequency bands, but it
will rather be a convergence of upcoming technological trends driven by
exciting, underlying services. In this regard, we first identify the primary
drivers of 6G systems, in terms of applications and accompanying technological
trends. Then, we propose a new set of service classes and expose their target
6G performance requirements. We then identify the enabling technologies for the
introduced 6G services and outline a comprehensive research agenda that
leverages those technologies. We conclude by providing concrete recommendations
for the roadmap toward 6G. Ultimately, the intent of this article is to serve
as a basis for stimulating more out-of-the-box research around 6G.
","Walid Saad|Mehdi Bennis|Mingzhe Chen","http://arxiv.org/pdf/1902.10265v2",""
"33","1910.11658v1","Quantum Networks For Open Science","  The United States Department of Energy convened the Quantum Networks for Open
Science (QNOS) Workshop in September 2018. The workshop was primarily focused
on quantum networks optimized for scientific applications with the expectation
that the resulting quantum networks could be extended to lay the groundwork for
a generalized network that will evolve into a quantum internet.
","Thomas Ndousse-Fetter|Nicholas Peters|Warren Grice|Prem Kumar|Tom Chapuran|Saikat Guha|Scott Hamilton|Inder Monga|Ray Newell|Andrei Nomerotski|Don Towsley|Ben Yoo","http://arxiv.org/pdf/1910.11658v1",""
"34","1904.00237v2","A decentralized method for making sensor measurements tamper-proof to
  support open science applications","  Open science has become a synonym for modern, digital and inclusive science.
Inclusion does not stop at open access. Inclusion also requires transparency
through open datasets and the right and ability to take part in the knowledge
creation process. This implies new challenges for digital libraries. Citizens
should be able to contribute data in a curatable form to advance science. At
the same time, this data should be verifiable and attributable to its owner.
Our research project focusses on securing and attributing incoming data streams
from sensors. Our contribution is twofold. First, we analyze the promises of
open science measurement data and point out how Blockchain technology changed
the circumstances for data measurement in science projects using sensors.
Second, we present an open hardware project capable of securing the integrity
of data directly from the source using cryptographic methods. By using
inexpensive modular components and open source software, we lower the barrier
for participation in open science projects. We show how time series of
measurement values using sensors, e.g., temperature, current, and vibration
measurements, can be verifiably and immutably stored. The approach we propose
enables time series data to be stored in a tamper-proof manner and securely
timestamped on a blockchain to prevent any subsequent modification.
","Patrick Wortner|Moritz Schubotz|Corinna Breitinger|Stephan Leible|Bela Gipp","http://arxiv.org/pdf/1904.00237v2",""
"35","1904.05383v4","Trusted CI Experiences in Cybersecurity and Service to Open Science","  This article describes experiences and lessons learned from the Trusted CI
project, funded by the US National Science Foundation to serve the community as
the NSF Cybersecurity Center of Excellence. Trusted CI is an effort to address
cybersecurity for the open science community through a single organization that
provides leadership, training, consulting, and knowledge to that community. The
article describes the experiences and lessons learned of Trusted CI regarding
both cybersecurity for open science and managing the process of providing
centralized services to a broad and diverse community.
","Andrew Adams|Kay Avila|Jim Basney|Dana Brunson|Robert Cowles|Jeannette Dopheide|Terry Fleury|Elisa Heymann|Florence Hudson|Craig Jackson|Ryan Kiser|Mark Krenz|Jim Marsteller|Barton P. Miller|Sean Peisert|Scott Russell|Susan Sons|Von Welch|John Zage","http://arxiv.org/pdf/1904.05383v4","http://dx.doi.org/10.1145/3332186.3340601"
"36","1904.06499v3","Open Science in Software Engineering","  Open science describes the movement of making any research artefact available
to the public and includes, but is not limited to, open access, open data, and
open source. While open science is becoming generally accepted as a norm in
other scientific disciplines, in software engineering, we are still struggling
in adapting open science to the particularities of our discipline, rendering
progress in our scientific community cumbersome. In this chapter, we reflect
upon the essentials in open science for software engineering including what
open science is, why we should engage in it, and how we should do it. We
particularly draw from our experiences made as conference chairs implementing
open science initiatives and as researchers actively engaging in open science
to critically discuss challenges and pitfalls, and to address more advanced
topics such as how and under which conditions to share preprints, what
infrastructure and licence model to cover, or how do it within the limitations
of different reviewing models, such as double-blind reviewing. Our hope is to
help establishing a common ground and to contribute to make open science a norm
also in software engineering.
","Daniel Méndez Fernández|Daniel Graziotin|Stefan Wagner|Heidi Seibold","http://arxiv.org/pdf/1904.06499v3","http://dx.doi.org/10.1007/978-3-030-32489-6_17"
"37","1904.09270v1","The Rise of Internet of Things (IoT) in Big Healthcare Data: Review and
  Open research Issues","  Health is one of the sustainable development areas in all of the countries.
Internet of Things has a variety of use in this sector which was not studied
yet. The aim of this research is to prioritize IoT usage in the healthcare
sector to achieve sustainable development. The study is an applied descriptive
research according to data collection. As per the research methodology which is
FAHP, it is a single cross sectional survey research. After data collection,
the agreed paired comparison matrices, allocated to weighted criteria and the
priority of IoT usage were determined. Based on the research findings, the two
criteria of Economic Prosperity and Quality of Life achieved the highest
priority for IoT sustainable development in the healthcare sector. Moreover,
the top priorities for IoT in the area of health, according to the usage, were
identified as Ultraviolet Radiation, Dental Health and Fall Detection.
","Zainab Alansari|Safeeullah Soomro|Mohammad Riyaz Belgaum|Shahaboddin Shamshirband","http://arxiv.org/pdf/1904.09270v1","http://dx.doi.org/10.1007/978-981-10-6875-1_66"
"38","1905.06911v1","StashCache: A Distributed Caching Federation for the Open Science Grid","  Data distribution for opportunistic users is challenging as they neither own
the computing resources they are using or any nearby storage. Users are
motivated to use opportunistic computing to expand their data processing
capacity, but they require storage and fast networking to distribute data to
that processing. Since it requires significant management overhead, it is rare
for resource providers to allow opportunistic access to storage. Additionally,
in order to use opportunistic storage at several distributed sites, users
assume the responsibility to maintain their data. In this paper we present
StashCache, a distributed caching federation that enables opportunistic users
to utilize nearby opportunistic storage. StashCache is comprised of four
components: data origins, redirectors, caches, and clients. StashCache has been
deployed in the Open Science Grid for several years and has been used by many
projects. Caches are deployed in geographically distributed locations across
the U.S. and Europe. We will present the architecture of StashCache, as well as
utilization information of the infrastructure. We will also present performance
analysis comparing distributed HTTP Proxies vs StashCache.
","Derek Weitzel|Marian Zvada|Ilija Vukotic|Rob Gardner|Brian Bockelman|Mats Rynge|Edgar Fajardo Hernandez|Brian Lin|Matyas Selmeci","http://arxiv.org/pdf/1905.06911v1","http://dx.doi.org/10.1145/3332186.3332212"
"39","1911.01276v5","Digital Twin: Enabling Technologies, Challenges and Open Research","  Digital Twin technology is an emerging concept that has become the centre of
attention for industry and, in more recent years, academia. The advancements in
industry 4.0 concepts have facilitated its growth, particularly in the
manufacturing industry. The Digital Twin is defined extensively but is best
described as the effortless integration of data between a physical and virtual
machine in either direction. The challenges, applications, and enabling
technologies for Artificial Intelligence, Internet of Things (IoT) and Digital
Twins are presented. A review of publications relating to Digital Twins is
performed, producing a categorical review of recent papers. The review has
categorised them by research areas: manufacturing, healthcare and smart cities,
discussing a range of papers that reflect these areas and the current state of
research. The paper provides an assessment of the enabling technologies,
challenges and open research for Digital Twins.
","Aidan Fuller|Zhong Fan|Charles Day|Chris Barlow","http://arxiv.org/pdf/1911.01276v5","http://dx.doi.org/10.1109/ACCESS.2020.2998358"
"40","1911.02782v3","S2ORC: The Semantic Scholar Open Research Corpus","  We introduce S2ORC, a large corpus of 81.1M English-language academic papers
spanning many academic disciplines. The corpus consists of rich metadata, paper
abstracts, resolved bibliographic references, as well as structured full text
for 8.1M open access papers. Full text is annotated with automatically-detected
inline mentions of citations, figures, and tables, each linked to their
corresponding paper objects. In S2ORC, we aggregate papers from hundreds of
academic publishers and digital archives into a unified source, and create the
largest publicly-available collection of machine-readable academic text to
date. We hope this resource will facilitate research and development of tools
and tasks for text mining over academic text.
","Kyle Lo|Lucy Lu Wang|Mark Neumann|Rodney Kinney|Dan S. Weld","http://arxiv.org/pdf/1911.02782v3",""
"41","1912.10561v1","A Survey of NOMA: Current Status and Open Research Challenges","  Non-orthogonal multiple access (NOMA) has been considered as a study-item in
3GPP for 5G new radio (NR). However, it was decided not to continue with it as
a work-item, and to leave it for possible use in beyond 5G. In this paper, we
first review the discussions that ended in such decision. Particularly, we
present simulation comparisons between the NOMA and multi-user
multiple-input-multiple-output (MU-MIMO), where the possible gain of NOMA,
compared to MU-MIMO, is negligible. Then, we propose a number of methods to
reduce the implementation complexity and delay of both uplink (UL) and downlink
(DL) NOMA-based transmission, as different ways to improve its efficiency.
Here, particular attention is paid to reducing the receiver complexity, the
cost of hybrid automatic repeat request as well as the user pairing complexity.
As demonstrated, different smart techniques can be applied to improve the
energy efficiency and the end-to-end transmission delay of NOMA-based systems.
","Behrooz Makki|Krishna Chitti|Ali Behravan|Mohamed-Slim Alouini","http://arxiv.org/pdf/1912.10561v1",""
"42","2003.07687v1","Augmented reality as a tool for open science platform by research
  collaboration in virtual teams","  The provision of open science is defined as a general policy aimed at
overcoming the barriers that hinder the implementation of the European Research
Area (ERA). An open science foundation seeks to capture all the elements needed
for the functioning of ERA: research data, scientific instruments, ICT services
(connections, calculations, platforms, and specific studies such as portals).
Managing shared resources for the community of scholars maximizes the benefits
to society. In the field of digital infrastructure, this has already
demonstrated great benefits. It is expected that applying this principle to an
open science process will improve management by funding organizations in
collaboration with stakeholders through mechanisms such as public consultation.
This will increase the perception of joint ownership of the infrastructure. It
will also create clear and non-discriminatory access rules, along with a sense
of joint ownership that stimulates a higher level of participation,
collaboration and social reciprocity. The article deals with the concept of
open science. The concept of the European cloud of open science and its
structure are presented. According to the study, it has been shown that the
structure of the cloud of open science includes an augmented reality as an
open-science platform. An example of the practical application of this tool is
the general description of MaxWhere, developed by Hungarian scientists, and is
a platform of aggregates of individual 3D spaces.
","Mariya P. Shyshkina|Maiia V. Marienko","http://arxiv.org/pdf/2003.07687v1",""
"43","2004.06049v2","A Prospective Look: Key Enabling Technologies, Applications and Open
  Research Topics in 6G Networks","  The fifth generation (5G) mobile networks are envisaged to enable a plethora
of breakthrough advancements in wireless technologies, providing support of a
diverse set of services over a single platform. While the deployment of 5G
systems is scaling up globally, it is time to look ahead for beyond 5G systems.
This is driven by the emerging societal trends, calling for fully automated
systems and intelligent services supported by extended reality and haptics
communications. To accommodate the stringent requirements of their prospective
applications, which are data-driven and defined by extremely low-latency,
ultra-reliable, fast and seamless wireless connectivity, research initiatives
are currently focusing on a progressive roadmap towards the sixth generation
(6G) networks. In this article, we shed light on some of the major enabling
technologies for 6G, which are expected to revolutionize the fundamental
architectures of cellular networks and provide multiple homogeneous artificial
intelligence-empowered services, including distributed communications, control,
computing, sensing, and energy, from its core to its end nodes. Particularly,
this paper aims to answer several 6G framework related questions: What are the
driving forces for the development of 6G? How will the enabling technologies of
6G differ from those in 5G? What kind of applications and interactions will
they support which would not be supported by 5G? We address these questions by
presenting a profound study of the 6G vision and outlining five of its
disruptive technologies, i.e., terahertz communications, programmable
metasurfaces, drone-based communications, backscatter communications and
tactile internet, as well as their potential applications. Then, by leveraging
the state-of-the-art literature surveyed for each technology, we discuss their
requirements, key challenges, and open research problems.
","Lina Bariah|Lina Mohjazi|Sami Muhaidat|Paschalis C. Sofotasios|Gunes Karabulut Kurt|Halim Yanikomeroglu|Octavia A. Dobre","http://arxiv.org/pdf/2004.06049v2",""
"44","2004.05125v1","Rapidly Deploying a Neural Search Engine for the COVID-19 Open Research
  Dataset: Preliminary Thoughts and Lessons Learned","  We present the Neural Covidex, a search engine that exploits the latest
neural ranking architectures to provide information access to the COVID-19 Open
Research Dataset curated by the Allen Institute for AI. This web application
exists as part of a suite of tools that we have developed over the past few
weeks to help domain experts tackle the ongoing global pandemic. We hope that
improved information access capabilities to the scientific literature can
inform evidence-based decision making and insight generation. This paper
describes our initial efforts and offers a few thoughts about lessons we have
learned along the way.
","Edwin Zhang|Nikhil Gupta|Rodrigo Nogueira|Kyunghyun Cho|Jimmy Lin","http://arxiv.org/pdf/2004.05125v1",""
"45","2004.10706v4","CORD-19: The COVID-19 Open Research Dataset","  The COVID-19 Open Research Dataset (CORD-19) is a growing resource of
scientific papers on COVID-19 and related historical coronavirus research.
CORD-19 is designed to facilitate the development of text mining and
information retrieval systems over its rich collection of metadata and
structured full text papers. Since its release, CORD-19 has been downloaded
over 200K times and has served as the basis of many COVID-19 text mining and
discovery systems. In this article, we describe the mechanics of dataset
construction, highlighting challenges and key design decisions, provide an
overview of how CORD-19 has been used, and describe several shared tasks built
around the dataset. We hope this resource will continue to bring together the
computing community, biomedical experts, and policy makers in the search for
effective treatments and management policies for COVID-19.
","Lucy Lu Wang|Kyle Lo|Yoganand Chandrasekhar|Russell Reas|Jiangjiang Yang|Doug Burdick|Darrin Eide|Kathryn Funk|Yannis Katsis|Rodney Kinney|Yunyao Li|Ziyang Liu|William Merrill|Paul Mooney|Dewey Murdick|Devvret Rishi|Jerry Sheehan|Zhihong Shen|Brandon Stilson|Alex Wade|Kuansan Wang|Nancy Xin Ru Wang|Chris Wilhelm|Boya Xie|Douglas Raymond|Daniel S. Weld|Oren Etzioni|Sebastian Kohlmeier","http://arxiv.org/pdf/2004.10706v4",""
"46","2004.12018v1","Jupyter notebooks as discovery mechanisms for open science: Citation
  practices in the astronomy community","  Citing data and software is a means to give scholarly credit and to
facilitate access to research objects. Citation principles encourage authors to
provide full descriptions of objects, with stable links, in their papers. As
Jupyter notebooks aggregate data, software, and other objects, they may
facilitate or hinder citation, credit, and access to data and software. We
report on a study of references to Jupyter notebooks in astronomy over a 5-year
period (2014-2018). References increased rapidly, but fewer than half of the
references led to Jupyter notebooks that could be located and opened. Jupyter
notebooks appear better suited to supporting the research process than to
providing access to research objects. We recommend that authors cite individual
data and software objects, and that they stabilize any notebooks cited in
publications. Publishers should increase the number of citations allowed in
papers and employ descriptive metadata-rich citation styles that facilitate
credit and discovery.
","Morgan F. Wofford|Bernadette M. Boscoe|Christine L. Borgman|Irene V. Pasquetto|Milena S. Golshan","http://arxiv.org/pdf/2004.12018v1","http://dx.doi.org/10.1109/MCSE.2019.2932067"
"47","2005.02367v5","CODA-19: Using a Non-Expert Crowd to Annotate Research Aspects on
  10,000+ Abstracts in the COVID-19 Open Research Dataset","  This paper introduces CODA-19, a human-annotated dataset that codes the
Background, Purpose, Method, Finding/Contribution, and Other sections of 10,966
English abstracts in the COVID-19 Open Research Dataset. CODA-19 was created by
248 crowd workers from Amazon Mechanical Turk within 10 days, and achieved
labeling quality comparable to that of experts. Each abstract was annotated by
nine different workers, and the final labels were acquired by majority vote.
The inter-annotator agreement (Cohen's kappa) between the crowd and the
biomedical expert (0.741) is comparable to inter-expert agreement (0.788).
CODA-19's labels have an accuracy of 82.2% when compared to the biomedical
expert's labels, while the accuracy between experts was 85.0%. Reliable human
annotations help scientists access and integrate the rapidly accelerating
coronavirus literature, and also serve as the battery of AI/NLP research, but
obtaining expert annotations can be slow. We demonstrated that a non-expert
crowd can be rapidly employed at scale to join the fight against COVID-19.
","Ting-Hao 'Kenneth' Huang|Chieh-Yang Huang|Chien-Kuang Cornelia Ding|Yen-Chia Hsu|C. Lee Giles","http://arxiv.org/pdf/2005.02367v5",""
"48","2005.08823v1","A Semantically Enriched Dataset based on Biomedical NER for the COVID19
  Open Research Dataset Challenge","  Research into COVID-19 is a big challenge and highly relevant at the moment.
New tools are required to assist medical experts in their research with
relevant and valuable information. The COVID-19 Open Research Dataset Challenge
(CORD-19) is a ""call to action"" for computer scientists to develop these
innovative tools. Many of these applications are empowered by entity
information, i. e. knowing which entities are used within a sentence. For this
paper, we have developed a pipeline upon the latest Named Entity Recognition
tools for Chemicals, Diseases, Genes and Species. We apply our pipeline to the
COVID-19 research challenge and share the resulting entity mentions with the
community.
","Hermann Kroll|Jan Pirklbauer|Johannes Ruthmann|Wolf-Tilo Balke","http://arxiv.org/pdf/2005.08823v1",""
"49","2005.10334v1","Requirements Analysis for an Open Research Knowledge Graph","  Current science communication has a number of drawbacks and bottlenecks which
have been subject of discussion lately: Among others, the rising number of
published articles makes it nearly impossible to get an overview of the state
of the art in a certain field, or reproducibility is hampered by fixed-length,
document-based publications which normally cannot cover all details of a
research work. Recently, several initiatives have proposed knowledge graphs
(KGs) for organising scientific information as a solution to many of the
current issues. The focus of these proposals is, however, usually restricted to
very specific use cases. In this paper, we aim to transcend this limited
perspective by presenting a comprehensive analysis of requirements for an Open
Research Knowledge Graph (ORKG) by (a) collecting daily core tasks of a
scientist, (b) establishing their consequential requirements for a KG-based
system, (c) identifying overlaps and specificities, and their coverage in
current solutions. As a result, we map necessary and desirable requirements for
successful KG-based science communication, derive implications and outline
possible solutions.
","Arthur Brack|Anett Hoppe|Markus Stocker|Sören Auer|Ralph Ewerth","http://arxiv.org/pdf/2005.10334v1","http://dx.doi.org/10.1007/978-3-030-54956-5_1"
"50","2005.14021v1","Knowledge Utilization and Open Science Policies: Noble aims that ensure
  quality research or Ordering discoveries like a pizza?","  Open Science has been a rising theme in the landscape of science policy in
recent years. The goal is to make research that emerges from publicly funded
science to become findable, accessible, interoperable and reusable (FAIR) for
use by other researchers. Knowledge utilization policies aim to efficiently
make scientific knowledge beneficial for society at large. This paper
demonstrates how Astronomy aspires to be open and transparent given their
criteria for high research quality, which aim at pushing knowledge forward and
clear communication of findings. However, the use of quantitative metrics in
research evaluation puts pressure on the researcher, such that taking the extra
time for transparent publishing of data and results is difficult, given that
astronomers are not rewarded for the quality of research papers, but rather
their quantity. This paper explores the current mode of openness in Astronomy
and how incentives due to funding, publication practices and indicators affect
this field. The paper concludes with some recommendations on how policies such
as making science more open have the potential to contribute to scientific
quality in Astronomy.
","Julia Heuritsch","http://arxiv.org/pdf/2005.14021v1",""
"51","2006.03091v1","The FRET-based structural dynamics challenge -- community contributions
  to consistent and open science practices","  Single-molecule F\""{o}rster resonance energy transfer (smFRET) has become a
mainstream technique for probing biomolecular structural dynamics. The rapid
and wide adoption of the technique by an ever-increasing number of groups has
generated many improvements and variations in the technique itself, in methods
for sample preparation and characterization, in analysis of the data from such
experiments, and in analysis codes and algorithms. Recently, several labs that
employ smFRET have joined forces to try to bring the smFRET community together
in adopting a consensus on how to perform experiments and analyze results for
achieving quantitative structural information. These recent efforts include
multi-lab blind-tests to assess the accuracy and precision of smFRET between
different labs using different procedures, the formal assembly of the FRET
community and development of smFRET procedures to be considered for entries in
the wwPDB. Here we delve into the different approaches and viewpoints in the
field. This position paper describes the current ""state-of-the field"", points
to unresolved methodological issues for quantitative structural studies,
provides a set of 'soft recommendations' about which an emerging consensus
exists, and a list of resources that are openly available. To make further
progress, we strongly encourage 'open science' practices. We hope that this
position paper will provide a roadmap for newcomers to the field, as well as a
reference for seasoned practitioners.
","Eitan Lerner|Benjamin Ambrose|Anders Barth|Victoria Birkedal|Scott C. Blanchard|Richard Borner|Thorben Cordes|Timothy D. Craggs|Taekjip Ha|Gilad Haran|Thorsten Hugel|Antonino Ingargiola|Achillefs Kapanidis|Don C. Lamb|Ted Laurence|Nam ki Lee|Edward A. Lemke|Emmanuel Margeat|Jens Michaelis|Xavier Michalet|Daniel Nettels|Thomas-Otavio Peulen|Benjamin Schuler|Claus A. M. Seidel|Hamid So-leimaninejad|Shimon Weiss","http://arxiv.org/pdf/2006.03091v1","http://dx.doi.org/10.7554/eLife.60416"
"52","2006.13733v1","Operational Research Literature as a Use Case for the Open Research
  Knowledge Graph","  The Open Research Knowledge Graph (ORKG) provides machine-actionable access
to scholarly literature that habitually is written in prose. Following the FAIR
principles, the ORKG makes traditional, human-coded knowledge findable,
accessible, interoperable, and reusable in a structured manner in accordance
with the Linked Open Data paradigm. At the moment, in ORKG papers are described
manually, but in the long run the semantic depth of the literature at scale
needs automation. Operational Research is a suitable test case for this vision
because the mathematical field and, hence, its publication habits are highly
structured: A mundane problem is formulated as a mathematical model, solved or
approximated numerically, and evaluated systematically. We study the existing
literature with respect to the Assembly Line Balancing Problem and derive a
semantic description in accordance with the ORKG. Eventually, selected papers
are ingested to test the semantic description and refine it further.
","Mila Runnwerth|Markus Stocker|Sören Auer","http://arxiv.org/pdf/2006.13733v1","http://dx.doi.org/10.1007/978-3-030-52200-1_32"
"53","2007.06222v1","Toward porting Astrophysics Visual Analytics Services to the European
  Open Science Cloud","  The European Open Science Cloud (EOSC) aims to create a federated environment
for hosting and processing research data to support science in all disciplines
without geographical boundaries, such that data, software, methods and
publications can be shared as part of an Open Science community of practice.
This work presents the ongoing activities related to the implementation of
visual analytics services, integrated into EOSC, towards addressing the diverse
astrophysics user communities needs. These services rely on visualisation to
manage the data life cycle process under FAIR principles, integrating data
processing for imaging and multidimensional map creation and mosaicing, and
applying machine learning techniques for detection of structures in large scale
multidimensional maps.
","Eva Sciacca|Fabio Vitello|Ugo Becciani|Cristobal Bordiu|Filomena Bufano|Antonio Calanducci|Alessandro Costa|Mario Raciti|Simone Riggi","http://arxiv.org/pdf/2007.06222v1","http://dx.doi.org/10.1007/978-3-030-52243-8_43"
"54","2007.07846v1","Covidex: Neural Ranking Models and Keyword Search Infrastructure for the
  COVID-19 Open Research Dataset","  We present Covidex, a search engine that exploits the latest neural ranking
models to provide information access to the COVID-19 Open Research Dataset
curated by the Allen Institute for AI. Our system has been online and serving
users since late March 2020. The Covidex is the user application component of
our three-pronged strategy to develop technologies for helping domain experts
tackle the ongoing global pandemic. In addition, we provide robust and
easy-to-use keyword search infrastructure that exploits mature fusion-based
methods as well as standalone neural ranking models that can be incorporated
into other applications. These techniques have been evaluated in the ongoing
TREC-COVID challenge: Our infrastructure and baselines have been adopted by
many participants, including some of the highest-scoring runs in rounds 1, 2,
and 3. In round 3, we report the highest-scoring run that takes advantage of
previous training data and the second-highest fully automatic run.
","Edwin Zhang|Nikhil Gupta|Raphael Tang|Xiao Han|Ronak Pradeep|Kuang Lu|Yue Zhang|Rodrigo Nogueira|Kyunghyun Cho|Hui Fang|Jimmy Lin","http://arxiv.org/pdf/2007.07846v1",""
"55","2009.07399v1","High-Performance Mining of COVID-19 Open Research Datasets for Text
  Classification and Insights in Cloud Computing Environments","  COVID-19 global pandemic is an unprecedented health crisis. Since the
outbreak, many researchers around the world have produced an extensive
collection of literatures. For the research community and the general public to
digest, it is crucial to analyse the text and provide insights in a timely
manner, which requires a considerable amount of computational power. Clouding
computing has been widely adopted in academia and industry in recent years. In
particular, hybrid cloud is gaining popularity since its two-fold benefits:
utilising existing resource to save cost and using additional cloud service
providers to gain assess to extra computing resources on demand. In this paper,
we developed a system utilising the Aneka PaaS middleware with parallel
processing and multi-cloud capability to accelerate the ETL and article
categorising process using machine learning technology on a hybrid cloud. The
result is then persisted for further referencing, searching and visualising.
Our performance evaluation shows that the system can help with reducing
processing time and achieving linear scalability. Beyond COVID-19, the
application might be used directly in broader scholarly article indexing and
analysing.
","Jie Zhao|Maria A. Rodriguez|Rajkumar Buyya","http://arxiv.org/pdf/2009.07399v1",""
"56","2009.08801v1","SciBERT-based Semantification of Bioassays in the Open Research
  Knowledge Graph","  As a novel contribution to the problem of semantifying biological assays, in
this paper, we propose a neural-network-based approach to automatically
semantify, thereby structure, unstructured bioassay text descriptions.
Experimental evaluations, to this end, show promise as the neural-based
semantification significantly outperforms a naive frequency-based baseline
approach. Specifically, the neural method attains 72% F1 versus 47% F1 from the
frequency-based method.
","Marco Anteghini|Jennifer D'Souza|Vitor A. P. Martins dos Santos|Sören Auer","http://arxiv.org/pdf/2009.08801v1",""
"57","2009.07642v1","Representing Semantified Biological Assays in the Open Research
  Knowledge Graph","  In the biotechnology and biomedical domains, recent text mining efforts
advocate for machine-interpretable, and preferably, semantified, documentation
formats of laboratory processes. This includes wet-lab protocols, (in)organic
materials synthesis reactions, genetic manipulations and procedures for faster
computer-mediated analysis and predictions. Herein, we present our work on the
representation of semantified bioassays in the Open Research Knowledge Graph
(ORKG). In particular, we describe a semantification system work-in-progress to
generate, automatically and quickly, the critical semantified bioassay data
mass needed to foster a consistent user audience to adopt the ORKG for
recording their bioassays and facilitate the organisation of research,
according to FAIR principles.
","Marco Anteghini|Jennifer D'Souza|Vitor A. P. Martins dos Santos|Sören Auer","http://arxiv.org/pdf/2009.07642v1",""
"58","2009.08395v1","A Multimodal Memes Classification: A Survey and Open Research Issues","  Memes are graphics and text overlapped so that together they present concepts
that become dubious if one of them is absent. It is spread mostly on social
media platforms, in the form of jokes, sarcasm, motivating, etc. After the
success of BERT in Natural Language Processing (NLP), researchers inclined to
Visual-Linguistic (VL) multimodal problems like memes classification, image
captioning, Visual Question Answering (VQA), and many more. Unfortunately, many
memes get uploaded each day on social media platforms that need automatic
censoring to curb misinformation and hate. Recently, this issue has attracted
the attention of researchers and practitioners. State-of-the-art methods that
performed significantly on other VL dataset, tends to fail on memes
classification. In this context, this work aims to conduct a comprehensive
study on memes classification, generally on the VL multimodal problems and
cutting edge solutions. We propose a generalized framework for VL problems. We
cover the early and next-generation works on VL problems. Finally, we identify
and articulate several open research issues and challenges. This is the first
study that presents the generalized view of the advanced classification
techniques concerning memes classification to the best of our knowledge. We
believe this study presents a clear road-map for the Machine Learning (ML)
research community to implement and enhance memes classification techniques.
","Tariq Habib Afridi|Aftab Alam|Muhammad Numan Khan|Jawad Khan|Young-Koo Lee","http://arxiv.org/pdf/2009.08395v1",""
"59","2010.04508v2","Towards an Open Science definition as a political and legal framework:
  on the sharing and dissemination of research outputs","  It is widely recognised nowadays that there is no single, accepted, unified
definition of Open Science, which motivates our proposal of an Open Science
definition as a political and legal framework where research outputs are shared
and disseminated in order to be rendered visible, accessible, reusable is
developed, standing over the concepts enhanced by the Budapest Open Science
Initiative (BOAI), and by the Free/Open Source Software (FOSS) and Open data
movements. We elaborate this proposal through a detailed analysis of some
selected EC policies and laws as well as of the function of research evaluation
practices. The legal aspects considered in our examination include, in
particular, the study of the role of licenses in the context of the
dissemination of research outputs.
","Teresa Gomez-Diaz|Tomas Recio","http://arxiv.org/pdf/2010.04508v2","http://dx.doi.org/10.5281/zenodo.4577066"
"60","2011.02833v3","Digital Twins: State of the Art Theory and Practice, Challenges, and
  Open Research Questions","  Digital Twin was introduced over a decade ago, as an innovative
all-encompassing tool, with perceived benefits including real-time monitoring,
simulation and forecasting. However, the theoretical framework and practical
implementations of digital twins (DT) are still far from this vision. Although
successful implementations exist, sufficient implementation details are not
publicly available, therefore it is difficult to assess their effectiveness,
draw comparisons and jointly advance the DT methodology. This work explores the
various DT features and current approaches, the shortcomings and reasons behind
the delay in the implementation and adoption of digital twin. Advancements in
machine learning, internet of things and big data have contributed hugely to
the improvements in DT with regards to its real-time monitoring and forecasting
properties. Despite this progress and individual company-based efforts, certain
research gaps exist in the field, which have caused delay in the widespread
adoption of this concept. We reviewed relevant works and identified that the
major reasons for this delay are the lack of a universal reference framework,
domain dependence, security concerns of shared data, reliance of digital twin
on other technologies, and lack of quantitative metrics. We define the
necessary components of a digital twin required for a universal reference
framework, which also validate its uniqueness as a concept compared to similar
concepts like simulation, autonomous systems, etc. This work further assesses
the digital twin applications in different domains and the current state of
machine learning and big data in it. It thus answers and identifies novel
research questions, both of which will help to better understand and advance
the theory and practice of digital twins.
","Angira Sharma|Edward Kosasih|Jie Zhang|Alexandra Brintrup|Anisoara Calinescu","http://arxiv.org/pdf/2011.02833v3",""
"61","2011.07807v1","Video Big Data Analytics in the Cloud: A Reference Architecture, Survey,
  Opportunities, and Open Research Issues","  The proliferation of multimedia devices over the Internet of Things (IoT)
generates an unprecedented amount of data. Consequently, the world has stepped
into the era of big data. Recently, on the rise of distributed computing
technologies, video big data analytics in the cloud has attracted the attention
of researchers and practitioners. The current technology and market trends
demand an efficient framework for video big data analytics. However, the
current work is too limited to provide a complete survey of recent research
work on video big data analytics in the cloud, including the management and
analysis of a large amount of video data, the challenges, opportunities, and
promising research directions. To serve this purpose, we present this study,
which conducts a broad overview of the state-of-the-art literature on video big
data analytics in the cloud. It also aims to bridge the gap among large-scale
video analytics challenges, big data solutions, and cloud computing. In this
study, we clarify the basic nomenclatures that govern the video analytics
domain and the characteristics of video big data while establishing its
relationship with cloud computing. We propose a service-oriented layered
reference architecture for intelligent video big data analytics in the cloud.
Then, a comprehensive and keen review has been conducted to examine
cutting-edge research trends in video big data analytics. Finally, we identify
and articulate several open research issues and challenges, which have been
raised by the deployment of big data technologies in the cloud for video big
data analytics. To the best of our knowledge, this is the first study that
presents the generalized view of the video big data analytics in the cloud.
This paper provides the research studies and technologies advancing video
analyses in the era of big data and cloud computing.
","Aftab Alam|Irfan Ullah|Young-Koo Lee","http://arxiv.org/pdf/2011.07807v1","http://dx.doi.org/10.1109/ACCESS.2020.3017135"
"62","2011.14995v1","Adapting LIGO workflows to run in the Open Science Grid","  During the first observation run the LIGO collaboration needed to offload
some of its most, intense CPU workflows from its dedicated computing sites to
opportunistic resources. Open Science Grid enabled LIGO to run PyCbC, RIFT and
Bayeswave workflows to seamlessly run in a combination of owned and
opportunistic resources. One of the challenges is enabling the workflows to use
several heterogeneous resources in a coordinated and effective way.
","Edgar Fajardo|Frank Wuerthwein|Brian Bockelman|Miron Livny|Greg Thain|James Alexander Clark|Peter Couvares|Josh Willis","http://arxiv.org/pdf/2011.14995v1",""
"63","2012.11534v2","ESCAPE -- addressing Open Science challenges","  ESCAPE (European Science Cluster of Astronomy & Particle physics ESFRI
research infrastructures) is an EU H2020 project that addresses the Open
Science challenges shared by the astrophysics and and accelerator-based physics
and nuclear physics ESFRI projects and landmarks. This project is embedded in
the context of the European Open Science Cloud (EOSC) and involves activities
to develop a prototype Data Lake and Science Platform, as well as support of an
Open Source Software Repository, connection of the Virtual Observatory
framework to EOSC, and engaging the public in citizen science. In this poster
paper we provide a brief overview of the project and the results presented at
ADASS.
","Mark G. Allen|Giovanni Lamanna|Xavier Espinal|Kay Graf|Michiel van Haarlem|Stephen Serjeant|Ian Bird|Elena Cuoco|Jayesh Wagh","http://arxiv.org/pdf/2012.11534v2",""
"64","2012.12958v1","Privacy Preservation for Wireless Sensor Networks in Healthcare: State
  of the Art, and Open Research Challenges","  The advent of miniature biosensors has generated numerous opportunities for
deploying wireless sensor networks in healthcare. However, an important barrier
is that acceptance by healthcare stakeholders is influenced by the
effectiveness of privacy safeguards for personal and intimate information which
is collected and transmitted over the air, within and beyond these networks. In
particular, these networks are progressing beyond traditional sensors, towards
also using multimedia sensors, which raise further privacy concerns.
Paradoxically, less research has addressed privacy protection, compared to
security. Nevertheless, privacy protection has gradually evolved from being
assumed an implicit by-product of security measures, and it is maturing into a
research concern in its own right. However, further technical and
socio-technical advances are needed. As a contribution towards galvanising
further research, the hallmarks of this paper include: (i) a literature survey
explicitly anchored on privacy preservation, it is underpinned by untangling
privacy goals from security goals, to avoid mixing privacy and security
concerns, as is often the case in other papers; (ii) a critical survey of
privacy preservation services for wireless sensor networks in healthcare,
including threat analysis and assessment methodologies; it also offers
classification trees for the multifaceted challenge of privacy protection in
healthcare, and for privacy threats, attacks and countermeasures; (iii) a
discussion of technical advances complemented by reflection over the
implications of regulatory frameworks; (iv) a discussion of open research
challenges, leading onto offers of directions for future research towards
unlocking the door onto privacy protection which is appropriate for healthcare
in the twenty-first century.
","Yasmine N. M. Saleh|Claude C. Chibelushi|Ayman A. Abdel-Hamid|Abdel-Hamid Soliman","http://arxiv.org/pdf/2012.12958v1",""
"65","2101.00250v1","Interplay between RIS and AI in Wireless Communications: Fundamentals,
  Architectures, Applications, and Open Research Problems","  Future wireless communication networks are expected to fulfill the
unprecedented performance requirements to support our highly digitized and
globally data-driven society. Various technological challenges must be overcome
to achieve our goal. Among many potential technologies, reconfigurable
intelligent surface (RIS) and artificial intelligence (AI) have attracted
extensive attention, thereby leading to a proliferation of studies for
utilizing them in wireless communication systems. The RIS-based wireless
communication frameworks and AI-enabled technologies, two of the promising
technologies for the sixth-generation networks, interact and promote with each
other, striving to collaboratively create a controllable, intelligent,
reconfigurable, and programmable wireless propagation environment. This paper
explores the road to implementing the combination of RIS and AI; specifically,
integrating AI-enabled technologies into RIS-based frameworks for maximizing
the practicality of RIS to facilitate the realization of smart radio
propagation environments, elaborated from shallow to deep insights. We begin
with the basic concept and fundamental characteristics of RIS, followed by the
overview of the research status of RIS. Then, we analyze the inevitable trend
of RIS to be combined with AI. In particular, we focus on recent research about
RIS-based architectures embedded with AI, elucidating from the intelligent
structures and systems of metamaterials to the AI-embedded RIS-assisted
wireless communication systems. Finally, the challenges and potential of the
topic are discussed.
","Jinghe Wang|Wankai Tang|Yu Han|Shi Jin|Xiao Li|Chao-Kai Wen|Qiang Cheng|Tie Jun Cui","http://arxiv.org/pdf/2101.00250v1","http://dx.doi.org/10.1109/JSAC.2021.3087259"
"66","2101.06751v2","The KM3NeT Open Science System","  The KM3NeT neutrino detectors are currently under construction at two
locations in the Mediterranean Sea, aiming to detect the Cherenkov light
generated by high-energy relativistic charged particles in sea water. The
KM3NeT collaboration will produce scientific data valuable both for the
astrophysics and neutrino physics communities as well as for the Earth and Sea
science community. An Open Science Portal and infrastructure are under
development to provide public access to open KM3NeT data, software and
services. In this contribution, the current architecture, interfaces and usage
examples are presented.
","Jutta Schnabel|Tamas Gal|Zineb Aly","http://arxiv.org/pdf/2101.06751v2",""
"67","2102.06021v1","Analysing the Requirements for an Open Research Knowledge Graph: Use
  Cases, Quality Requirements and Construction Strategies","  Current science communication has a number of drawbacks and bottlenecks which
have been subject of discussion lately: Among others, the rising number of
published articles makes it nearly impossible to get a full overview of the
state of the art in a certain field, or reproducibility is hampered by
fixed-length, document-based publications which normally cannot cover all
details of a research work. Recently, several initiatives have proposed
knowledge graphs (KG) for organising scientific information as a solution to
many of the current issues. The focus of these proposals is, however, usually
restricted to very specific use cases. In this paper, we aim to transcend this
limited perspective and present a comprehensive analysis of requirements for an
Open Research Knowledge Graph (ORKG) by (a) collecting and reviewing daily core
tasks of a scientist, (b) establishing their consequential requirements for a
KG-based system, (c) identifying overlaps and specificities, and their coverage
in current solutions. As a result, we map necessary and desirable requirements
for successful KG-based science communication, derive implications, and outline
possible solutions.
","Arthur Brack|Anett Hoppe|Markus Stocker|Sören Auer|Ralph Ewerth","http://arxiv.org/pdf/2102.06021v1",""
"68","2103.08334v1","The Virtual Observatory Ecosystem Facing the European Open Science Cloud","  The International Virtual Observatory Alliance (IVOA) has developed and
built, in the last two decades, an ecosystem of distributed resources,
interoperable and based upon open shared technological standards. In doing so
the IVOA has anticipated, putting into practice for the astrophysical domain,
the ideas of FAIR-ness of data and service resources and the Open-ness of
sharing scientific results, leveraging on the underlying open standards
required to fill the above. In Europe, efforts in supporting and developing the
ecosystem proposed by the IVOA specifications has been provided by a continuous
set of EU funded projects up to current H2020 ESCAPE ESFRI cluster. In the
meantime, in the last years, Europe has realised the importance of promoting
the Open Science approach for the research communities and started the European
Open Science Cloud (EOSC) project to create a distributed environment for
research data, services and communities. In this framework the European VO
community, had to face the move from the interoperability scenario in the
astrophysics domain into a larger audience perspective that includes a
cross-domain FAIR approach. Within the ESCAPE project the CEVO Work Package
(Connecting ESFRI to EOSC through the VO) has one task to deal with this
integration challenge: a challenge where an existing, mature, distributed
e-infrastructure has to be matched to a forming, more general architecture.
CEVO started its works in the first months of 2019 and has already worked on
the integration of the VO Registry into the EOSC e-infrastructure. This
contribution reports on the first year and a half of integration activities,
that involve applications, services and resources being aware of the VO
scenario and compatible with the EOSC architecture.
","Marco Molinaro|Mark Allen|Françoise Genova|André Schaaff|Margarida Castro Neves|Markus Demleitner|Sara Bertocco|Dave Morris|François Bonnarel|Stelios Voutsinas|Catherine Boisson|Giuliano Taffoni","http://arxiv.org/pdf/2103.08334v1","http://dx.doi.org/10.1117/12.2562322"
"69","2105.02351v1","Content4All Open Research Sign Language Translation Datasets","  Computational sign language research lacks the large-scale datasets that
enables the creation of useful reallife applications. To date, most research
has been limited to prototype systems on small domains of discourse, e.g.
weather forecasts. To address this issue and to push the field forward, we
release six datasets comprised of 190 hours of footage on the larger domain of
news. From this, 20 hours of footage have been annotated by Deaf experts and
interpreters and is made publicly available for research purposes. In this
paper, we share the dataset collection process and tools developed to enable
the alignment of sign language video and subtitles, as well as baseline
translation results to underpin future research.
","Necati Cihan Camgoz|Ben Saunders|Guillaume Rochette|Marco Giovanelli|Giacomo Inches|Robin Nachtrab-Ribback|Richard Bowden","http://arxiv.org/pdf/2105.02351v1",""
"70","2106.10207v2","Distributed Deep Learning in Open Collaborations","  Modern deep learning applications require increasingly more compute to train
state-of-the-art models. To address this demand, large corporations and
institutions use dedicated High-Performance Computing clusters, whose
construction and maintenance are both environmentally costly and well beyond
the budget of most organizations. As a result, some research directions become
the exclusive domain of a few large industrial and even fewer academic actors.
To alleviate this disparity, smaller groups may pool their computational
resources and run collaborative experiments that benefit all participants. This
paradigm, known as grid- or volunteer computing, has seen successful
applications in numerous scientific areas. However, using this approach for
machine learning is difficult due to high latency, asymmetric bandwidth, and
several challenges unique to volunteer computing. In this work, we carefully
analyze these constraints and propose a novel algorithmic framework designed
specifically for collaborative training. We demonstrate the effectiveness of
our approach for SwAV and ALBERT pretraining in realistic conditions and
achieve performance comparable to traditional setups at a fraction of the cost.
Finally, we provide a detailed report of successful collaborative language
model pretraining with 40 participants.
","Michael Diskin|Alexey Bukhtiyarov|Max Ryabinin|Lucile Saulnier|Quentin Lhoest|Anton Sinitsin|Dmitry Popov|Dmitry Pyrkin|Maxim Kashirin|Alexander Borzunov|Albert Villanova del Moral|Denis Mazur|Ilia Kobelev|Yacine Jernite|Thomas Wolf|Gennady Pekhimenko","http://arxiv.org/pdf/2106.10207v2",""
"71","2107.09619v1","Open-Science Platform for Computational Materials Science: AiiDA and the
  Materials Cloud","  We discuss here our vision for an Open-Science platform for computational
Materials Science. Such a platform needs to rely on three pillars, consisting
of 1) open data generation tools (including the simulation codes, the
scientific workflows and the infrastructure for automation and
provenance-tracking); 2) an open integration platform where these tools
interact in an easily accessible way and computations are coordinated by
automated workflows; and 3) support for seamless code and data sharing through
portals that are FAIR-compliant and compatible with data-management plans. As a
practical implementation, we show how such a platform in a few examples and
focusing in particular on the combination of the AiiDA infrastructure and the
Materials Cloud web portal.
","Giovanni Pizzi","http://arxiv.org/pdf/2107.09619v1","http://dx.doi.org/10.1007/978-3-319-42913-7_64-1"
"72","2107.13473v3","The Portiloop: a deep learning-based open science tool for closed-loop
  brain stimulation","  Closed-loop brain stimulation refers to capturing neurophysiological measures
such as electroencephalography (EEG), quickly identifying neural events of
interest, and producing auditory, magnetic or electrical stimulation so as to
interact with brain processes precisely. It is a promising new method for
fundamental neuroscience and perhaps for clinical applications such as
restoring degraded memory function; however, existing tools are expensive,
cumbersome, and offer limited experimental flexibility. In this article, we
propose the Portiloop, a deep learning-based, portable and low-cost closed-loop
stimulation system able to target specific brain oscillations. We first
document open-hardware implementations that can be constructed from
commercially available components. We also provide a fast, lightweight neural
network model and an exploration algorithm that automatically optimizes the
model hyperparameters to the desired brain oscillation. Finally, we validate
the technology on a challenging test case of real-time sleep spindle detection,
with results comparable to off-line expert performance on the Massive Online
Data Annotation spindle dataset (MODA; group consensus). Software and plans are
available to the community as an open science initiative to encourage further
development and advance closed-loop neuroscience research.
","Nicolas Valenchon|Yann Bouteiller|Hugo R. Jourde|Xavier L'Heureux|Milo Sobral|Emily B. J. Coffey|Giovanni Beltrame","http://arxiv.org/pdf/2107.13473v3","http://dx.doi.org/10.1371/journal.pone.0270696"
"73","2107.14652v1","Meeting the challenge of Open Science in KM3NeT","  In the upcoming decades, the KM3NeT detectors will produce valuable data that
can be used in various scientific contexts from astro- and particle physics to
environmental and Earth and Sea science. Based on the Open Science policy
established by the KM3NeT Collaboration, several efforts to offer science-ready
data, foster common analysis approaches and publish open source software are
currently pursued. In this contribution, ongoing projects focusing on the
exchange of high-level data and simulation derivatives, production of particle
event simulations and establishment of an integrated computing environment
supporting an open-science focused workflow will be discussed.
","Jutta Schnabel|Piotr Kalaczyński|Cristiano Bozza|Tamas Gal","http://arxiv.org/pdf/2107.14652v1","http://dx.doi.org/10.1088/1748-0221/16/10/C10002"
"74","2108.05085v1","Researcher or Crowd Member? Why not both! The Open Research Knowledge
  Graph for Applying and Communicating CrowdRE Research","  In recent decades, there has been a major shift towards improved digital
access to scholarly works. However, even now that these works are available in
digital form, they remain document-based, making it difficult to communicate
the knowledge they contain. The next logical step is to extend these works with
more flexible, fine-grained, semantic, and context-sensitive representations of
scholarly knowledge. The Open Research Knowledge Graph (ORKG) is a platform
that structures and interlinks scholarly knowledge, relying on crowdsourced
contributions from researchers (as a crowd) to acquire, curate, publish, and
process this knowledge. In this experience report, we consider the ORKG in the
context of Crowd-based Requirements Engineering (CrowdRE) from two
perspectives: (1) As CrowdRE researchers, we investigate how the ORKG
practically applies CrowdRE techniques to involve scholars in its development
to make it align better with their academic work. We determined that the ORKG
readily provides social and financial incentives, feedback elicitation
channels, and support for context and usage monitoring, but that there is
improvement potential regarding automated user feedback analyses and a holistic
CrowdRE approach. (2) As crowd members, we explore how the ORKG can be used to
communicate scholarly knowledge about CrowdRE research. For this purpose, we
curated qualitative and quantitative scholarly knowledge in the ORKG based on
papers contained in two previously published systematic literature reviews
(SLRs) on CrowdRE. This knowledge can be explored and compared interactively,
and with more data than what the SLRs originally contained. Therefore, the ORKG
improves access and communication of the scholarly knowledge about CrowdRE
research. For both perspectives, we found the ORKG to be a useful multi-tool
for CrowdRE research.
","Oliver Karras|Eduard C. Groen|Javed Ali Khan|Sören Auer","http://arxiv.org/pdf/2108.05085v1",""
"75","2108.12922v1","Continuous Systematic Literature Review: An Approach for Open Science","  Systematic Literature Reviews (SLRs) play an important role in the
Evidence-Based Software Engineering scenario. With the advance of the computer
science field and the growth of research publications, new evidence
continuously arises. This fact impacts directly on the purpose of keeping SLRs
up-to-date which could lead researchers to obsolete conclusions or decisions
about a research problem or investigation. Creating and maintaining SLRs
up-to-date demand a significant effort due to several reasons such as the rapid
increase in the amount of evidence, limitation of available databases and lack
of detailed protocol documentation and data availability. Conventionally, in
software engineering SLRs are not updated or updated intermittently leaving
gaps between updates during which time the SLR may be missing important new
research. In order to address these issues, we propose the concept, process and
tooling support of Continuous Systematic Literature Review (CSLR) in SE aiming
to keep SLRs constantly updated with the promotion of open science practices.
This positional paper summarizes our proposal and approach under development.
","Bianca Minetto Napoleão|Fabio Petrillo|Sylvain Hallé","http://arxiv.org/pdf/2108.12922v1",""
"76","2109.10159v1","From MANET to people-centric networking: milestones and open research
  challenges","  In this paper we discuss the state of the art of (mobile) multi-hop ad hoc
networking with the aim to present the current status of the research
activities and identify the consolidated research areas, with limited research
opportunities, and the hot and emerging research areas for which further
research is required. We start by briefly discussing the MANET paradigm, and
why the research on MANET protocols is now a cold research topic. Then we
analyze the active research areas. Specifically, after discussing the
wireless-network technologies we analyze four successful ad hoc networking
paradigms, mesh, opportunistic, vehicular networks, and sensor networks that
emerged from the MANET world. We also present the new research directions in
the multi-hop ad hoc networking field: people-centric networking, triggered by
the increasing penetration of the smartphones in everyday life, which is
generating a people-centric revolution in computing and communications.
","Marco Conti|Chiara Boldrini|Salil S. Kanhere|Enzo Mingozzi|Elena Pagani|Pedro M. Ruiz|Mohamed Younis","http://arxiv.org/pdf/2109.10159v1","http://dx.doi.org/10.1016/j.comcom.2015.09.007"
"77","2110.00888v2","Promoting Open Science Through Research Data Management","  Data management, which encompasses activities and strategies related to the
storage, organization, and description of data and other research materials,
helps ensure the usability of datasets -- both for the original research team
and for others. When contextualized as part of a research workflow, data
management practices can provide an avenue for promoting other practices,
including those related to reproducibility and those that fall under the
umbrella of open science. Not all research data needs to be shared, but all
should be well managed to establish a record of the research process.
","John A. Borghi|Ana E. Van Gulick","http://arxiv.org/pdf/2110.00888v2",""
"78","2112.14957v1","Tiansuan Constellation: An Open Research Platform","  Satellite network is the first step of interstellar voyages. It can provide
global Internet connectivity everywhere on earth, where most areas cannot
access the Internet by the terrestrial infrastructure due to the geographic
accessibility and high cost. The space industry experiences a rise in large
low-earth-orbit satellite constellations to achieve universal connectivity. The
research community is also urgent to do some leading research to bridge the
connectivity divide. Researchers now conduct their work by simulation, which is
far from enough. However, experiments on real satellites are blocked by the
high threshold of space technology, such as deployment cost and unknown risks.
To solve the above dilemma, we are eager to contribute to the universal
connectivity and build an open research platform, Tiansuan constellation to
support experiments on real satellite networks. We discuss the potential
research topics that would benefit from Tiansuan constellation. We provide two
case studies that have already deployed in two experimental satellites of
Tiansuan constellation.
","Shangguang Wang|Qing Li|Mengwei Xu|Xiao Ma|Ao Zhou|Qibo Sun","http://arxiv.org/pdf/2112.14957v1",""
"79","2201.05455v1","Service Function Chaining in 5G & Beyond Networks: Challenges and Open
  Research Issues","  Service Function Chaining (SFC) is a trending paradigm, which has helped to
introduce unseen flexibility in telecom networks. Network service providers, as
well as big network infrastructure providers, are competing to offer
personalized services for their customers. Hence, added value services require
the invocation of various elementary functions called Service Functions (SFs).
The SFC concept composes and imposes the order in which SFs are invoked for a
particular service. Emerging technologies such as Software Defined Networking
and Network Function Virtualization support the dynamic creation and management
of SFC. Even though SFC is an active technical area where several aspects were
already standardized and many SFC architecture flavors are currently deployed,
yet some challenges and open issues are still to be solved. In this paper, we
present different research problems related to SFC and investigate several key
challenges that should be addressed to realize more reliable SFC operations.
","H. Hantouti|N. Benamar|T. Taleb","http://arxiv.org/pdf/2201.05455v1",""
"80","2201.02256v2","LA-CoNGA physics: an Open Science Collaboration in Advanced Physics
  between Latin-America and Europe","  LA-CoNGA physics (for Latin-American alliance for Capacity buildiNG in
Advance physics) is an ERASMUS+ project aiming to support the modernization of
university infrastructure and its pedagogical offer in advanced physics in four
Latin-American countries: Colombia, Ecuador, Per\'u and Venezuela. This project
is co-funded by the Education, Audiovisual and Culture Executive Agency of the
European Commission. This virtual teaching and research network comprises three
partner universities in Europe and eight in Latin America; high-level
scientific partners (CEA, CERN, CNRS, DESY, ICTP), and several and two
industrial partners.
  During 2019 we prepared the syllabuses and selected the lecturers. In 2020
the strict lockdowns modified our pedagogical strategies. The planned model --
an eight-node network of universities made-up by local groups for discussions
-- was transformed into low-quality home participation. We simplified the
connectivity requirements to the minimum bandwidth to operate remote labs. We
also changed the lecture interaction and evaluation model, balancing the
teamwork on course projects and continuous evaluation based on class exercises.
Despite the lockdown scenario, we managed to support the needs of our
instrumentation and computing courses thanks to the contribution and enthusiasm
of our partners. With the support of 30 instructors, we gave 100 lectures to 67
students in four countries. We are now promoting the second cohort due to start
in January 2022
","Jesús Peña-Rodríguez|Luis A. Núñez","http://arxiv.org/pdf/2201.02256v2",""
"81","2203.08809v1","Broadening the scope of Education, Career and Open Science in HEP","  High Energy Particle Physics (HEP) faces challenges over the coming decades
with a need to attract young people to the field and STEM careers, as well as a
need to recognize, promote and sustain those in the field who are making
important contributions to the research effort across the many specialties
needed to deliver the science. Such skills can also serve as attractors for
students who may not want to pursue a PhD in HEP but use them as a springboard
to other STEM careers. This paper reviews the challenges and develops
strategies to correct the disparities to help transform the particle physics
field into a stronger and more diverse ecosystem of talent and expertise, with
the expectation of long-lasting scientific and societal benefits.
","Sudhir Malik|David DeMuth|Sijbrand de Jong|Randal Ruchti|Savannah Thais|Guillermo Fidalgo|Ken Heller|Mathew Muether|Minerba Betancourt|Meenakshi Narain|Tiffany R. Lewis|Kyle Cranmer|Gordon Watts","http://arxiv.org/pdf/2203.08809v1",""
"82","2203.14574v1","The Digitalization of Bioassays in the Open Research Knowledge Graph","  Background: Recent years are seeing a growing impetus in the semantification
of scholarly knowledge at the fine-grained level of scientific entities in
knowledge graphs. The Open Research Knowledge Graph (ORKG)
https://www.orkg.org/ represents an important step in this direction, with
thousands of scholarly contributions as structured, fine-grained,
machine-readable data. There is a need, however, to engender change in
traditional community practices of recording contributions as unstructured,
non-machine-readable text. For this in turn, there is a strong need for AI
tools designed for scientists that permit easy and accurate semantification of
their scholarly contributions. We present one such tool, ORKG-assays.
Implementation: ORKG-assays is a freely available AI micro-service in ORKG
written in Python designed to assist scientists obtain semantified bioassays as
a set of triples. It uses an AI-based clustering algorithm which on
gold-standard evaluations over 900 bioassays with 5,514 unique property-value
pairs for 103 predicates shows competitive performance. Results and Discussion:
As a result, semantified assay collections can be surveyed on the ORKG platform
via tabulation or chart-based visualizations of key property values of the
chemicals and compounds offering smart knowledge access to biochemists and
pharmaceutical researchers in the advancement of drug development.
","Jennifer D'Souza|Anita Monteverdi|Muhammad Haris|Marco Anteghini|Kheir Eddine Farfar|Markus Stocker|Vitor A. P. Martins dos Santos|Sören Auer","http://arxiv.org/pdf/2203.14574v1",""
"83","2203.14579v1","Computer Science Named Entity Recognition in the Open Research Knowledge
  Graph","  Domain-specific named entity recognition (NER) on Computer Science (CS)
scholarly articles is an information extraction task that is arguably more
challenging for the various annotation aims that can beset the task and has
been less studied than NER in the general domain. Given that significant
progress has been made on NER, we believe that scholarly domain-specific NER
will receive increasing attention in the years to come. Currently, progress on
CS NER -- the focus of this work -- is hampered in part by its recency and the
lack of a standardized annotation aim for scientific entities/terms. This work
proposes a standardized task by defining a set of seven contribution-centric
scholarly entities for CS NER viz., research problem, solution, resource,
language, tool, method, and dataset. Following which, its main contributions
are: combines existing CS NER resources that maintain their annotation focus on
the set or subset of contribution-centric scholarly entities we consider;
further, noting the need for big data to train neural NER models, this work
additionally supplies thousands of contribution-centric entity annotations from
article titles and abstracts, thus releasing a cumulative large novel resource
for CS NER; and, finally, trains a sequence labeling CS NER model inspired
after state-of-the-art neural architectures from the general domain NER task.
Throughout the work, several practical considerations are made which can be
useful to information technology designers of the digital libraries.
","Jennifer D'Souza|Sören Auer","http://arxiv.org/pdf/2203.14579v1",""
"84","2204.05779v1","Towards Polyglot Data Stores -- Overview and Open Research Questions","  Nowadays, data-intensive applications face the problem of handling
heterogeneous data with sometimes mutually exclusive use cases and soft
non-functional goals such as consistency and availability. Since no single
platform copes everything, various stores (RDBMS, NewSQL, NoSQL) for different
workloads and use-cases have been developed. However, since each store is only
a specialization, this motivates progress in polyglot data management emerged
new systems called Mult- and Polystores. They are trying to access different
stores transparently and combine their capabilities to achieve one or multiple
given use-cases. This paper describes representative real-world use cases for
data-intensive applications (OLTP and OLAP). It derives a set of requirements
for polyglot data stores. Subsequently, we discuss the properties of selected
Multi- and Polystores and evaluate them based on given needs illustrated by
three common application use cases. We classify them into functional features,
query processing technique, architecture and adaptivity and reveal a lack of
capabilities, especially in changing conditions tightly integration. Finally,
we outline the benefits and drawbacks of the surveyed systems and propose
future research directions and current challenges in this area.
","Daniel Glake|Felix Kiehn|Mareike Schmidt|Fabian Panse|Norbert Ritter","http://arxiv.org/pdf/2204.05779v1",""
"85","2205.11262v2","From BeyondPlanck to Cosmoglobe: Open Science, Reproducibility, and Data
  Longevity","  The BeyondPlanck and Cosmoglobe collaborations have implemented the first
integrated Bayesian end-to-end analysis pipeline for CMB experiments. The
primary long-term motivation for this work is to develop a common analysis
platform that supports efficient global joint analysis of complementary radio,
microwave, and sub-millimeter experiments. A strict prerequisite for this to
succeed is broad participation from the CMB community, and two foundational
aspects of the program are therefore reproducibility and Open Science. In this
paper, we discuss our efforts toward this aim. We also discuss measures toward
facilitating easy code and data distribution, community-based code
documentation, user-friendly compilation procedures, etc. This work represents
the first publicly released end-to-end CMB analysis pipeline that includes raw
data, source code, parameter files, and documentation. We argue that such a
complete pipeline release should be a requirement for all major future and
publicly-funded CMB experiments, noting that a full public release
significantly increases data longevity by ensuring that the data quality can be
improved whenever better processing techniques, complementary datasets, or more
computing power become available, and thereby also taxpayers' value for money;
providing only raw data and final products is not sufficient to guarantee full
reproducibility in the future.
","S. Gerakakis|M. Brilenkov|M. Ieronymaki|M. San|D. J. Watts|K. J. Andersen|R. Aurlien|R. Banerji|A. Basyrov|M. Bersanelli|S. Bertocco|M. Carbone|L. P. L. Colombo|H. K. Eriksen|J. R. Eskilt|M. K. Foss|C. Franceschet|U. Fuskeland|S. Galeotta|M. Galloway|E. Gjerløw|B. Hensley|D. Herman|M. Iacobellis|H. T. Ihle|J. B. Jewell|A. Karakci|E. Keihänen|R. Keskitalo|J. G. S. Lunde|G. Maggio|D. Maino|M. Maris|S. Paradiso|M. Reinecke|N. -O. Stutzer|A. -S. Suur-Uski|T. L. Svalheim|D. Tavagnacco|H. Thommesen|I. K. Wehus|A. Zacchei","http://arxiv.org/pdf/2205.11262v2",""
"86","2206.01439v1","Open Research Knowledge Graph:A System Walkthrough","  Despite improved digital access to scholarly literature in the last decades,
the fundamental principles of scholarly communication remain unchanged and
continue to be largely document-based. Scholarly knowledge remains locked in
representations that are inadequate for machine processing. The Open Research
Knowledge Graph (ORKG) is an infrastructure for representing, curating and
exploring scholarly knowledge in a machine actionable manner. We demonstrate
the core functionality of ORKG for representing research contributions
published in scholarly articles. A video of the demonstration and the system
are available online.
","Mohamad Yaser Jaradeh|Allard Oelen|Manuel Prinz|Markus Stocker|Sören Auer","http://arxiv.org/pdf/2206.01439v1","http://dx.doi.org/10.1007/978-3-030-30760-8_31"
"87","2206.03926v2","Enabling Portability and Reusability of Open Science Infrastructures","  This paper presents a methodology for designing a containerized and
distributed open science infrastructure to simplify its reusability,
replicability, and portability in different environments. The methodology is
depicted in a step-by-step schema based on four main phases: (1) Analysis, (2)
Design, (3) Definition, and (4) Managing and provisioning. We accompany the
description of each step with existing technologies and concrete examples of
application.
","Giuseppe Grieco|Ivan Heibi|Arcangelo Massari|Arianna Moretti|Silvio Peroni","http://arxiv.org/pdf/2206.03926v2","http://dx.doi.org/10.1007/978-3-031-16802-4_36"
"88","2207.02775v2","Open Science and Authorship of Supplementary Material. Evidence from a
  Research Community","  Authorship of scientific articles has profoundly changed from early science
until now. While once upon a time a paper was authored by a handful of authors,
scientific collaborations are much more prominent on average nowadays. As
authorship (and citation) is essentially the primary reward mechanism according
to the traditional research evaluation frameworks, it turned out to be a rather
hot-button topic from which a significant portion of academic disputes stems.
However, the novel Open Science practices could be an opportunity to disrupt
such dynamics and diversify the credit of the different scientific contributors
involved in the diverse phases of the lifecycle of the same research effort. In
fact, a paper and research data (or software) contextually published could
exhibit different authorship to give credit to the various contributors right
where it feels most appropriate. As a preliminary study, in this paper, we
leverage the wealth of information contained in Open Science Graphs, such as
OpenAIRE, and conduct a focused analysis on a subset of publications with
supplementary material drawn from the European Marine Science (MES) research
community. The results are promising and suggest our hypothesis is worth
exploring further as we registered in 22% of the cases substantial variations
between the authors participating in the publication and the authors
participating in the supplementary dataset (or software), thus posing the
premises for a longitudinal, large-scale analysis of the phenomenon.
","Andrea Mannocci|Ornella Irrera|Paolo Manghi","http://arxiv.org/pdf/2207.02775v2","http://dx.doi.org/10.5281/zenodo.6975411"
"89","2207.03121v1","Will open science change authorship for good? Towards a quantitative
  analysis","  Authorship of scientific articles has profoundly changed from early science
until now. If once upon a time a paper was authored by a handful of authors,
scientific collaborations are much more prominent on average nowadays. As
authorship (and citation) is essentially the primary reward mechanism according
to the traditional research evaluation frameworks, it turned to be a rather
hot-button topic from which a significant portion of academic disputes stems.
However, the novel Open Science practices could be an opportunity to disrupt
such dynamics and diversify the credit of the different scientific contributors
involved in the diverse phases of the lifecycle of the same research effort. In
fact, a paper and research data (or software) contextually published could
exhibit different authorship to give credit to the various contributors right
where it feels most appropriate. We argue that this can be computationally
analysed by taking advantage of the wealth of information in model Open Science
Graphs. Such a study can pave the way to understand better the dynamics and
patterns of authorship in linked literature, research data and software, and
how they evolved over the years.
","Andrea Mannocci|Ornella Irrera|Paolo Manghi","http://arxiv.org/pdf/2207.03121v1",""
"90","2207.07478v1","Yourfeed: Towards open science and interoperable systems for social
  media","  Existing social media platforms (SMPs) make it incredibly difficult for
researchers to conduct studies on social media, which in turn has created a
knowledge gap between academia and industry about the effects of platform
design on user behavior. To close the gap, we introduce Yourfeed, a research
tool for conducting ecologically valid social media research. We introduce the
platform architecture, as well key opportunities such as assessing the effects
of exposure of content on downstream beliefs and attitudes, measuring
attentional exposure via dwell time, and evaluating heterogeneous newsfeed
algorithms. We discuss the underlying philosophy of interoperability for social
media and future developments for the platform.
","Ziv Epstein|Hause Lin","http://arxiv.org/pdf/2207.07478v1",""
"91","2208.04682v1","Playing catch-up in building an open research commons","  On August 2, 2021 a group of concerned scientists and US funding agency and
federal government officials met for an informal discussion to explore the
value and need for a well-coordinated US Open Research Commons (ORC); an
interoperable collection of data and compute resources within both the public
and private sectors which are easy to use and accessible to all.
","Philip E. Bourne|Vivien Bonazzi|Amy Brand|Bonnie Carroll|Ian Foster|Ramanathan V. Guha|Robert Hanisch|Sallie Ann Keller|Mary Lee Kennedy|Christine Kirkpatrick|Barend Mons|Sarah M. Nusser|Michael Stebbins|George Strawn|Alex Szalay","http://arxiv.org/pdf/2208.04682v1","http://dx.doi.org/10.1126/science.abo5947"
"92","2208.00510v1","Public Sector Platforms going Open: Creating and Growing an Ecosystem
  with Open Collaborative Development","  Background: By creating ecosystems around platforms of Open Source Software
(OSS) and Open Data (OD), and adopting open collaborative development
practices, platform providers may exploit open innovation benefits. However,
adopting such practices in a traditionally closed organization is a maturity
process that we hypothesize cannot be undergone without friction.
  Objective: This study aims to investigate what challenges may occur for a
newly-turned platform provider in the public sector, aiming to adopt open
collaborative practices to create an ecosystem around the development of the
underpinning platform.
  Method: An exploratory case-study is conducted at a Swedish public sector
platform provider, which is creating an ecosystem around OSS and OD, related to
the labor market. Data is collected through interviews, document studies, and
prolonged engagement.
  Results: Findings highlight a fear among developers of being publicly
questioned for their work, as they represent a government agency undergoing
constant scrutiny. Issue trackers, roadmaps, and development processes are
generally closed, while multiple channels are used for communication, causing
internal and external confusion. Some developers are reluctant to communicate
externally as they believe it interferes with their work. Lack of health
metrics limits possibilities to follow ecosystem growth and for actors to make
investment decisions. Further, an autonomous team structure is reported to
complicate internal communication and enforcement of the common vision, as well
as collaboration. A set of interventions for addressing the challenges are
proposed, based on related work.
  Conclusions: We conclude that several cultural, organizational, and
process-related challenges may reside, and by understanding these early on,
platform providers can be preemptive in their work of building healthy
ecosystems.
","Johan Linåker|Per Runeson","http://arxiv.org/pdf/2208.00510v1","http://dx.doi.org/10.1145/3412569.3412572"
"93","2209.00216v1","Using HCI in Cross-Disciplinary Teams: A Case Study of Academic
  Collaboration in HCI-Health Teams in the US Using a Team Science Perspective","  Human-centered computing research has been increasingly applied to address
important challenges in the health domain. Conducting research in
cross-disciplinary teams can come with a lot of challenges in integrating
knowledge across fields. Yet, we do not know what challenges HCI researchers
encounter in building collaborations with health researchers, and how these
researchers negotiate challenges while balancing their professional goals. We
interviewed 17 early- and mid-career HCI faculty working in the United States
who conducted research in collaboration with health researchers. Drawing from a
Team Science framework, we share participants' lived experiences and identify
major challenges that HCI researchers encounter when finding, collaborating
with, and negotiating with health collaborators when building technologies. We
propose ways to better support research collaboration aimed at designing
technologies using human-centered computing approaches. This includes
strategies to support HCI researchers at individual, institutional, research
community, and funding agencies levels through tools to translate disciplinary
approaches. We suggest institutional policies to support HCI researchers
through training, networking, and promotion.
","Elena Agapie|Shefali Haldar|Sharmaine Galvez Poblete","http://arxiv.org/pdf/2209.00216v1","http://dx.doi.org/10.1145/3555610"
"94","2210.02034v1","Clustering Semantic Predicates in the Open Research Knowledge Graph","  When semantically describing knowledge graphs (KGs), users have to make a
critical choice of a vocabulary (i.e. predicates and resources). The success of
KG building is determined by the convergence of shared vocabularies so that
meaning can be established. The typical lifecycle for a new KG construction can
be defined as follows: nascent phases of graph construction experience
terminology divergence, while later phases of graph construction experience
terminology convergence and reuse. In this paper, we describe our approach
tailoring two AI-based clustering algorithms for recommending predicates (in
RDF statements) about resources in the Open Research Knowledge Graph (ORKG)
https://orkg.org/. Such a service to recommend existing predicates to semantify
new incoming data of scholarly publications is of paramount importance for
fostering terminology convergence in the ORKG. Our experiments show very
promising results: a high precision with relatively high recall in linear
runtime performance. Furthermore, this work offers novel insights into the
predicate groups that automatically accrue loosely as generic semantification
patterns for semantification of scholarly knowledge spanning 44 research
fields.
","Omar Arab Oghli|Jennifer D'Souza|Sören Auer","http://arxiv.org/pdf/2210.02034v1",""
"95","2210.06413v1","EleutherAI: Going Beyond ""Open Science"" to ""Science in the Open""","  Over the past two years, EleutherAI has established itself as a radically
novel initiative aimed at both promoting open-source research and conducting
research in a transparent, openly accessible and collaborative manner.
EleutherAI's approach to research goes beyond transparency: by doing research
entirely in public, anyone in the world can observe and contribute at every
stage. Our work has been received positively and has resulted in several
high-impact projects in Natural Language Processing and other fields. In this
paper, we describe our experience doing public-facing machine learning
research, the benefits we believe this approach brings, and the pitfalls we
have encountered.
","Jason Phang|Herbie Bradley|Leo Gao|Louis Castricato|Stella Biderman","http://arxiv.org/pdf/2210.06413v1",""
"96","2210.12383v1","Stance Detection and Open Research Avenues","  This tutorial aims to cover the state-of-the-art on stance detection and
address open research avenues for interested researchers and practitioners.
Stance detection is a recent research topic where the stance towards a given
target or target set is determined based on the given content and there are
significant application opportunities of stance detection in various domains.
The tutorial comprises two parts where the first part outlines the fundamental
concepts, problems, approaches, and resources of stance detection, while the
second part covers open research avenues and application areas of stance
detection. The tutorial will be a useful guide for researchers and
practitioners of stance detection, social media analysis, information
retrieval, and natural language processing.
","Dilek Küçük|Fazli Can","http://arxiv.org/pdf/2210.12383v1",""
