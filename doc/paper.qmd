---
title: "Towards open collaboration in biomedical and health research: An open collaborative scoping review"
date: last-modified
author:
  - name: Mario Garcia
    affiliations:
      - ref: ku
  - name: Daniel B. Ibsen
    affiliations:
      - ref: sdca
      - ref: au
  - name: Luke W. Johnston
    affiliations:
      - ref: sdca
      - ref: au
affiliations:
  - id: sdca
    name: Steno Diabetes Center Aarhus
    country: Denmark
  - id: au
    name: Aarhus University
    country: Denmark
  - id: ku
    name: Copenhagen University
    country: Denmark
format:
  html: default
  typst:
    columns: 2
    papersize: a4
    margin:
      x: 2cm
      y: 2cm
format-links:
  - html
  - format: typst
    text: PDF
    icon: file-pdf
---

```{r setup}
#| include: false
knitr::opts_chunk$set(echo = FALSE)
library(targets)
library(readr)
library(dplyr)
library(glue)
library(stringr)
library(here)
source(here("R/openalex-search.R"))
search_terms_inline <- glue("({search_terms()$title_search}[title]) AND {search_terms()$general_search}")
reviewed_records <- tar_read(reviewed_article_records)

list_refs <- bib2df::bib2df(here("doc/reviewed-records.bib")) |>
  filter(CATEGORY != "COMMENT") |>
  glue_data("@{BIBTEXKEY}") |>
  str_c(collapse = ";") |>
  append("[", 0) |>
  append("]") |>
  str_c(collapse = "")
```

## Introduction

-   Open science and its role in research

-   Collaboration and its role in modern research

    -   collaboration spanning from lab, to research center to multiple
        centers
    -   collaboration needed to tackle limitations: small populations,
        lack of replication, etc
    -   re-inventing discovery: a new way of working in science

Scientific research now almost always requires working with other
people.

With the growing complexity and specialization in scientific practices
and methods, together with globalisation of health and environmental
issues, there is a great need for a paradigm shift in research
collaboration to be able to tackle these challenges.

-   Open collaboration - a combination of two themes

    -   definition
    -   big potential in science
    -   but there very few resources and examples of how to integrate
        open science into collaborations

We define open collaboration using the definition as found in
[@Forte2013]:

> "*an online environment that (a) supports the collective production of
> an artifact (b) through a technologically mediated collaboration
> platform (c) that presents a low barrier to entry and exit and (d)
> supports the emergence of persistent but malleable social
> structures.*"

With the increasing emphasis on and demand for science to be more open,
how we collaborate together is a key component to making science more
open from the start of any project. But how do we collaborate in an open
and transparent way? What are the best practices and tools we can use?
What is an ideal collaborative workflow and how close or far are we from
this ideal in reality?

### Aim

This scoping review will focus on current practices of open
collaboration and open science in relation to collaboration in the field
of biomedical and health research.

The specific aims of this scoping review are to:

-   Provide an overview of current practices of or opinions about
    research collaboration that follow basic open principles (e.g.,
    transparency, accessibility)

-   Summarize existing online tools and resources available to improve
    open collaboration in research

We've expanded on our original aims to include a **secondary aim** of
building an open source R-based pipeline for conducting scoping reviews.
The entire source code, as well as text and collaboration workflows, are
found on our GitHub repository
[`science-collective/scoping-review`](https://github.com/science-collective/scoping-review).

## Methods

The full protocol for this scoping review was uploaded to the Open
Science Framework [@Johnston2022]. As with any research project, things
evolve compared to what was originally intended or planned. In this
section we briefly re-state what we described in the protocol and
especially we describe what was changed from the protocol.

This is a scoping review, so we followed the framework described in
[@Arksey2005; @Levac2010] as well as the guidelines outlined in the
PRISMA-ScR statement [@Tricco2018].

### Deviations and challenges

We originally aimed to review individual archives that included PubMed,
bioRxiv, Scopus, and several others (the full list is in the protocol),
and to use R packages with web API connections to each of these sources
to programmatically extract the sources we wanted. However we
encountered substantial issues that completely changed how we actually
found and extracted the sources.

The first challenge we encountered was that, while most of the source
databases had (hypothetically) exposed APIs along with R packages
available to access them, they didn't always work well or had
complicated instructions for actually using them. For instance, the
preprint repositories bioRxiv and medRxiv didn't have an up to date R
package to access the preprints nor was the web API well described, nor
well designed. In order to effectively use it, you need to download the
entire database of preprints locally before being able to search for the
preprints you want. This made it effectively impossible to use.

The second challenge we encountered was a difference in results between
using the web API compared to using the web search interface for some
source databases. There were some small differences in search results
between the API query compared to the web-based query. We couldn't
identify or understand how these differences happened or way, though
they were quite small differences (less than 10 out of thousands of
scholarly works from the search results).

The third challenge we had was that some source databases, like the Web
of Science or Scopus, didn't work consistently between the authors. In
order to use the web API with the R packages, you need to generate a
token for authentication. However, for some authors, the token worked
fine, and for others, it didn't work at all, with no explanation or
error message. Given our interest in and aim for reproducibility, since
the same code ran differently between authors, we decided to exclude
these source databases.

We also originally intended on searching websites, blogs, and other
online resources, but it was very difficult to programmatically and
systemtically achieve this. So we ended up only searching for scholarly
work that were indexed in databases.

Given these challenges however, we unintentionally found
[OpenAlex](https://openalex.org/), which is a public database of
scholarly output that aggregates dozens of other source databases into
one, easy interface. This resource also has an R package called
`{openalexR}` [@Aria2024] with a very well designed interface, which
simplified or made redundant much of our code we wrote. It also made it
unnecessary to use the individual R packages for each source database
that we originally listed in the protocol. By using this archive, it
resolved all our challenges and barriers we had before. For a full list
of where they get their source data from, see their ["About the
data"](https://help.openalex.org/hc/en-us/articles/24397285563671-About-the-data)
page.

### Document selection

We developed the initial search strategy in consultation with a research
librarian. We collected the data via systematic searches of databases.
All authors were involved in reviewing the collected sources, from
reviewing the titles, to abstracts, and finally to full-text documents.

#### Information sources

We used OpenAlex, after encountering the challenges described above,
which is an open database with scholarly works from Microsoft Academic
Graph, Crossref, ORCID, ROR, DOAJ, Unpaywall, Pubmed, Pubmed Central,
the ISSN International Centre, Internet Archive, Web crawls,
subject-area and institutional repositories from arXiv and Zenodo.

#### Search terms

We used the following search terms when searching for scholarly work:

> `r search_terms_inline`

#### Inclusion and exclusion criteria

Inclusion criteria included any document where open collaboration
practices are not the primary focus. We relied on the definition of open
collaboration from [@Forte2013] in determining whether the records were
relevant. Any published document with reporting on current open
collaboration practices. Any published document with advice, guidance,
tools, and/or recommendations for improving open collaboration. Article
language in English. Exclusion criteria were documents that do not
report on specific open collaboration practices.

While we didn't change our search terms for the initial search and
extraction, we found that our search terms were not precise enough and
got many irrelevant scholarly works. The full R code to filter down the
search results is kept in the
[`R/exclusions.R`](https://github.com/science-collective/scoping-review/blob/main/R/exclusions.R)
file on the GitHub repository. For example, we extracted a large number
of scholarly works describing surgery such as "open wound", electronics
such as "open circuit", or environmental such as "open water" that we
had to post-process and exclude.

#### Charting procedure

At least two of us extracted data using a standardized and tested
template. Data regarding the data source (e.g., author, title,
publication year), open collaboration practices, and any other relevant
information, will be extracted. Extracted data will be summarized with
the descriptive analytical method described in [@Arksey2005], which is
aimed at identifying and summarizing different open collaboration
practices.

#### Search period

We were not able to dedicate as much time to this project as we had
initially planned, so our "stopping rule" for end date didn't match with
our actual search period. The actual end date for when we did the last
search results extraction was
[2024-02-29](https://github.com/science-collective/scoping-review/commit/f288d3f23d028e77ad1081e0cfe3583ce4dcb8b6).

The start date remained 5 years from our end date, which would have been
approximately 2019-02-28.

## Results

```{r}
#| echo: false
n_papers <- tar_read(records_title_only) |>
  nrow()

n_papers_title_review <- tar_read(records_after_title_exclusion) |>
  nrow()

n_papers_excluded_title_review <- n_papers - n_papers_title_review

n_titles_selected <- tar_read(titles_selected) |>
  nrow()

n_paper_full_text <- tar_read(abstracts_selected) |>
  nrow()

n_papers_selected_for_review <- read_csv(here("data", "article_characteristics.csv")) |>
  nrow()

# get table of study characteristics
article_characteristics <- read_csv(here("data", "article_characteristics.csv"))

n_guides <- article_characteristics |>
  filter(paper_type == "guide") |>
  nrow()

n_examples <- article_characteristics |>
  filter(paper_type == "example") |>
  nrow()

n_surveys <- article_characteristics |>
  filter(paper_type == "survey") |>
  nrow()

n_org <- article_characteristics |>
  filter(str_detect(themes, "organization")) |>
  nrow()

n_tools <- article_characteristics |>
  filter(str_detect(themes, "tools")) |>
  nrow()

n_education <- article_characteristics |>
  filter(str_detect(themes, "education")) |>
  nrow()

n_researcher <- article_characteristics |>
  filter(relevant_for == "researcher") |>
  nrow()

n_research_group <- article_characteristics |>
  filter(relevant_for == "research group") |>
  nrow()

n_research_center <- article_characteristics |>
  filter(relevant_for == "research center") |>
  nrow()

n_research_centers <- article_characteristics |>
  filter(relevant_for == "multiple centers") |>
  nrow()

n_research_field <- article_characteristics |>
  filter(relevant_for == "research field") |>
  nrow()
```

Our search yielded a total of `r n_papers` papers from which we filtered
`r n_papers_excluded_title_review` based on key words. Three reviewers
independently screened for the remaining `r n_papers_title_review`
titles from which we screened `r n_titles_selected` abstracts. Of these,
we assessed `r n_paper_full_text` full-text papers and we included
`r n_papers_selected_for_review` in the scoping review. These were
`r list_refs`. Two of the documents, while they passed our abstract
review, did not contain anything relevant when we reviewed the full text
[@Network2023a; @Yordanov2022], so we did not continue with charting
them.

The identified papers included `r n_guides` guides, `r n_examples`
examples and `r n_surveys` surveys.

`r n_org` of the `r n_papers_selected_for_review` papers included
recommendations for organizational practices to improve open
collaboration, `r n_tools` mentioned specific tools used for open
collaboration and `r n_education` highlighted educational needs. The
identified papers discussed open collaboration on different
organizational levels. `r n_researcher` were mostly directed at the
individual researcher, `r n_research_group` at a research group,
`r n_research_center` at a research center, `r n_research_centers` at
several research centers and `r n_research_field` at entire research
fields.

### Organizational practices of open collaboration

A common theme across organizational levels was the need for
infrastructure that promotes open collaboration, building a community of
collaborators and using online tools in accordance with the FAIR
(Findable, Accessible, Interoperable and Reusable) principles.

individual level - add text

laboratory/research group level - add text

For center-level one paper described how they transitioned their
neuroimaging research center toward more open and collaborative
practices. Their central focus was on creating more open code and data
to improve open collaboration [@Bush2022]. First, they used open-source
code, then standardized data formats, developed analysis pipelines,
shared data openly and developed protocols for data documentation. While
the organizational transition to improve open collaboration was
described, little was mentioned about how this has improved
collaborative work at the research center.

Interestingly, the level of organization with most papers was the
multi-center level with two papers from ManyPrimates [@Primates2019;
@ManyPrimates2021] and the Reproducibility Network [@Network2023]. In
this organization level the most urgent needs are: 1) identifying the
needs for open collaboration and 2) creating an infrastructure that
allows those needs to be satisfied. Indeed, Grange et al describes a
systematic review as an exploration of the needs for 11 UKRN centers to
implement open collaborative practives. In this paper they specially
focus on tools and resources among the 11 institutions and make clear
the need for starting to utilize open resources as a first step to adapt
open collaborative practices. The first paper from ManyPrimates
[@Primates2019] describes the start of another multi-center
collaboration, albeit from different institutions. It is interesting to
observe that in this situation, the complex issue is to start the
collaboration itself, which was ignited thanks to a symposium and an
email chain among future collaborators. In contrast, ManyPrimates could
quickly develop an open collaborative structure with relative ease in
comparison to changing an already solid infrastructure, like in the case
of @Network2023.

research field level

Here, we provide the specific needs for implementing open collaboration
in each organizational level, as well as describing the differences in
approaches among experiences between and across levels (Figure 2).

### Useful tools for open collaboration

### Educational needs

## Discussion

### Conclusions

## Contributions

<!-- TODO: add contributions according to: https://credit.niso.org/ -->

We originally had another author involved, HC, who is on the protocol
author list. Since the time we uploaded the protocol, she has moved out
of academia and could no longer participate in any work.

Otherwise, the remaining authors (MG, DBI, and LWJ) had the following
contribution roles, following the [CRediT](https://credit.niso.org/)
taxonomy of conceptualization, data curation, formal analysis,
investigation, methodology, writing - original draft, writing -
reviewing and editing, project administration, software, validation, and
visualization.
